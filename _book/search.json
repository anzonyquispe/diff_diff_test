[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "",
    "text": "1 Overview\nThis book provides a comprehensive comparison of difference-in-differences (DiD) estimators designed for staggered treatment adoption with heterogeneous treatment effects. We benchmark multiple R and Python packages using synthetic data following the simulation design from @callaway2021difference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.1 Background",
    "text": "1.1 Background\nTraditional two-way fixed effects (TWFE) estimators can produce biased estimates when:\n\nTreatment is adopted at different times across units (staggered rollout)\nTreatment effects vary over time or across cohorts (heterogeneous effects)\n\nSeveral new estimators have been developed to address these issues, including methods by:\n\nCallaway & Sant’Anna (2021): Group-time average treatment effects\nde Chaisemartin & D’Haultfoeuille (2020, 2024): Robust DiD with multiple periods\nSun & Abraham (2021): Interaction-weighted estimator\nBorusyak, Jaravel & Spiess (2024): Imputation-based approach",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#packages-compared",
    "href": "index.html#packages-compared",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.2 Packages Compared",
    "text": "1.2 Packages Compared\n\n1.2.1 R Packages\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nAuthors\nAvailable\n\n\n\n\ndid\nGroup-time ATT\nCallaway & Sant’Anna\nYes\n\n\nDIDmultiplegt / DIDmultiplegtDYN\nRobust DiD\nde Chaisemartin & D’Haultfoeuille\nYes\n\n\nfixest (sunab)\nInteraction-weighted\nSun & Abraham\nYes\n\n\ndidimputation\nImputation\nBorusyak, Jaravel & Spiess\nYes\n\n\n\n\n\n1.2.2 Python Packages\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nAuthors\nAvailable\n\n\n\n\ncsdid\nGroup-time ATT\nCallaway & Sant’Anna (port)\nYes\n\n\ndiff_diff\nMultiple methods\nCommunity\nYes\n\n\ndid_multiplegt_dyn\nRobust DiD\nde Chaisemartin & D’Haultfoeuille\nNo native Python package\n\n\npyfixest\nSun & Abraham via feols\nCommunity port\nYes (partial)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#simulation-design",
    "href": "index.html#simulation-design",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.3 Simulation Design",
    "text": "1.3 Simulation Design\nWe generate synthetic panel data with:\n\n1,000,000 units observed over 10 time periods\nStaggered treatment adoption: Units receive treatment at different times (2012, 2014, 2016, 2018) or never\nHeterogeneous treatment effects: Effects vary by treatment cohort and time since treatment\nUnit and time fixed effects\n\nThis design allows us to test whether each estimator correctly recovers the true treatment effects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#metrics",
    "href": "index.html#metrics",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.4 Metrics",
    "text": "1.4 Metrics\nFor each package, we report:\n\nExecution time: How long the estimation takes\nEstimated ATT: Compared to the true effect\nEvent study estimates: Dynamic effects by time relative to treatment\nMemory usage (where available)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.5 References",
    "text": "1.5 References\n\nCallaway, B., & Sant’Anna, P. H. (2021). Difference-in-differences with multiple time periods. Journal of Econometrics.\nde Chaisemartin, C., & D’Haultfoeuille, X. (2020). Two-way fixed effects estimators with heterogeneous treatment effects. American Economic Review.\nSun, L., & Abraham, S. (2021). Estimating dynamic treatment effects in event studies with heterogeneous treatment effects. Journal of Econometrics.\nBorusyak, K., Jaravel, X., & Spiess, J. (2024). Revisiting event study designs: Robust and efficient estimation. Review of Economic Studies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data_generation.html",
    "href": "data_generation.html",
    "title": "2  Synthetic Data Generation",
    "section": "",
    "text": "3 Data Generation Process\nWe generate synthetic panel data following the simulation design in Callaway & Sant’Anna (2021), with heterogeneous treatment effects that vary by:\nThis design creates a challenging test case where traditional TWFE would produce biased estimates.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#data-generating-process",
    "href": "data_generation.html#data-generating-process",
    "title": "2  Synthetic Data Generation",
    "section": "3.1 Data Generating Process",
    "text": "3.1 Data Generating Process\nThe outcome variable follows:\n\\[Y_{it} = \\alpha_i + \\lambda_t + \\tau_{g,t} \\cdot D_{it} + \\varepsilon_{it}\\]\nWhere:\n\n\\(\\alpha_i\\): Unit fixed effect\n\\(\\lambda_t\\): Time fixed effect\n\\(D_{it}\\): Treatment indicator (1 if unit \\(i\\) is treated at time \\(t\\))\n\\(\\tau_{g,t}\\): Treatment effect that varies by cohort \\(g\\) and calendar time \\(t\\)\n\\(\\varepsilon_{it} \\sim N(0, 1)\\): Idiosyncratic error\n\n\n3.1.1 Heterogeneous Treatment Effects\nWe specify treatment effects that:\n\nIncrease with time since treatment (dynamic effects)\nVary by treatment cohort (cohort heterogeneity)\n\n\\[\\tau_{g,t} = \\tau_0 + \\delta_g + \\gamma \\cdot (t - g)\\]\nWhere:\n\n\\(\\tau_0 = 1.0\\): Base treatment effect\n\\(\\delta_g\\): Cohort-specific shift (earlier cohorts have larger effects)\n\\(\\gamma = 0.1\\): Effect growth per period since treatment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#r-implementation",
    "href": "data_generation.html#r-implementation",
    "title": "2  Synthetic Data Generation",
    "section": "3.2 R Implementation",
    "text": "3.2 R Implementation\n\nset.seed(20240115)\n\n# Parameters\nn_units &lt;- 10000\nn_periods &lt;- 10\nbase_year &lt;- 2010\nyears &lt;- base_year:(base_year + n_periods - 1)\n\n# Treatment cohorts and probabilities\ntreat_cohorts &lt;- c(0, 2012, 2014, 2016, 2018)  # 0 = never treated\ncohort_probs &lt;- c(0.30, 0.20, 0.20, 0.20, 0.10)\n\n# Cohort-specific effect shifts (earlier cohorts have larger effects)\ncohort_effects &lt;- c(0, 0.5, 0.3, 0.1, 0.0)  # delta_g for each cohort\nnames(cohort_effects) &lt;- treat_cohorts\n\n# Base effect and dynamic effect parameter\ntau_0 &lt;- 1.0\ngamma &lt;- 0.1  # Effect growth per period\n\ncat(\"Generating panel data...\\n\")\n\nGenerating panel data...\n\ncat(\"Units:\", format(n_units, big.mark = \",\"), \"\\n\")\n\nUnits: 10,000 \n\ncat(\"Periods:\", n_periods, \"\\n\")\n\nPeriods: 10 \n\ncat(\"Total observations:\", format(n_units * n_periods, big.mark = \",\"), \"\\n\")\n\nTotal observations: 1e+05 \n\n# Generate unit-level data\nunit_data &lt;- data.frame(\n  id = 1:n_units,\n  g = sample(treat_cohorts, n_units, replace = TRUE, prob = cohort_probs),\n  unit_fe = rnorm(n_units, 0, 1)\n)\n\n# Time fixed effects\ntime_fe_vals &lt;- seq(-0.2, 0.2, length.out = n_periods)\nnames(time_fe_vals) &lt;- years\n\n# Expand to panel\ncat(\"Creating panel structure...\\n\")\n\nCreating panel structure...\n\npanel &lt;- expand.grid(id = 1:n_units, year = years)\npanel &lt;- merge(panel, unit_data, by = \"id\")\n\n# Add time fixed effects\npanel$time_fe &lt;- time_fe_vals[as.character(panel$year)]\n\n# Treatment indicator\npanel$treated &lt;- (panel$g &gt; 0) & (panel$year &gt;= panel$g)\n\n# Calculate heterogeneous treatment effects\n# tau_gt = tau_0 + delta_g + gamma * (t - g) for treated observations\npanel$tau_gt &lt;- 0\ntreated_idx &lt;- panel$treated\npanel$tau_gt[treated_idx] &lt;- tau_0 +\n  cohort_effects[as.character(panel$g[treated_idx])] +\n  gamma * (panel$year[treated_idx] - panel$g[treated_idx])\n\n# Generate outcome\npanel$y &lt;- 2 + panel$unit_fe + panel$time_fe +\n           panel$tau_gt * as.integer(panel$treated) +\n           rnorm(nrow(panel), 0, 1)\n\n# Rename for standard DiD notation\npanel$first_treat &lt;- panel$g\n\n# Sort for efficiency\npanel &lt;- panel[order(panel$id, panel$year), ]\n\ncat(\"\\nData generation complete!\\n\")\n\n\nData generation complete!\n\ncat(\"Treatment group distribution:\\n\")\n\nTreatment group distribution:\n\nprint(table(unit_data$g))\n\n\n   0 2012 2014 2016 2018 \n2991 2018 1928 2046 1017 \n\n# Save for use in subsequent chapters\nsaveRDS(panel, \"sim_data.rds\")\nwrite.csv(panel[, c(\"id\", \"year\", \"y\", \"first_treat\", \"treated\")],\n          \"sim_data.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#true-treatment-effects",
    "href": "data_generation.html#true-treatment-effects",
    "title": "2  Synthetic Data Generation",
    "section": "3.3 True Treatment Effects",
    "text": "3.3 True Treatment Effects\nLet’s calculate and display the true treatment effects for validation:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Calculate true ATT by cohort and event time\ntrue_effects &lt;- panel %&gt;%\n  filter(treated) %&gt;%\n  mutate(event_time = year - g) %&gt;%\n  group_by(g, event_time) %&gt;%\n  summarise(\n    true_att = mean(tau_gt),\n    n_obs = n(),\n    .groups = \"drop\"\n  )\n\n# Overall ATT\noverall_att &lt;- mean(panel$tau_gt[panel$treated])\ncat(\"Overall true ATT:\", round(overall_att, 4), \"\\n\\n\")\n\nOverall true ATT: 1.5869 \n\n# ATT by cohort\ncat(\"True ATT by cohort:\\n\")\n\nTrue ATT by cohort:\n\ncohort_att &lt;- true_effects %&gt;%\n  group_by(g) %&gt;%\n  summarise(true_att = mean(true_att), .groups = \"drop\")\nprint(cohort_att)\n\n# A tibble: 4 × 2\n      g true_att\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  2012     1.85\n2  2014     1.55\n3  2016     1.25\n4  2018     1.05\n\n# Dynamic effects (event study)\ncat(\"\\nTrue dynamic effects (averaged across cohorts):\\n\")\n\n\nTrue dynamic effects (averaged across cohorts):\n\ndynamic_effects &lt;- true_effects %&gt;%\n  group_by(event_time) %&gt;%\n  summarise(true_att = weighted.mean(true_att, n_obs), .groups = \"drop\")\nprint(dynamic_effects)\n\n# A tibble: 8 × 2\n  event_time true_att\n       &lt;dbl&gt;    &lt;dbl&gt;\n1          0     1.26\n2          1     1.36\n3          2     1.50\n4          3     1.60\n5          4     1.80\n6          5     1.90\n7          6     2.1 \n8          7     2.2 \n\n\n\nlibrary(ggplot2)\n\nggplot(dynamic_effects, aes(x = event_time, y = true_att)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Event Time (periods since treatment)\",\n    y = \"True ATT\",\n    title = \"True Dynamic Treatment Effects\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = -7:7)\n\n\n\n\nTrue treatment effects by event time",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#python-implementation",
    "href": "data_generation.html#python-implementation",
    "title": "2  Synthetic Data Generation",
    "section": "3.4 Python Implementation",
    "text": "3.4 Python Implementation\nFor Python packages, we’ll load the same data:\n\nimport numpy as np\nimport pandas as pd\n\n# Load the data generated by R for consistency\ndf = pd.read_csv(\"sim_data.csv\")\nprint(f\"Loaded {len(df):,} observations\")\nprint(f\"Units: {df['id'].nunique():,}\")\nprint(f\"Periods: {df['year'].nunique()}\")\nprint(f\"\\nTreatment cohort distribution:\")\nprint(df.groupby('first_treat')['id'].nunique())",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#data-summary",
    "href": "data_generation.html#data-summary",
    "title": "2  Synthetic Data Generation",
    "section": "3.5 Data Summary",
    "text": "3.5 Data Summary\n\ncat(\"Final dataset dimensions:\\n\")\n\nFinal dataset dimensions:\n\ncat(\"Rows:\", format(nrow(panel), big.mark = \",\"), \"\\n\")\n\nRows: 10,000,000 \n\ncat(\"Columns:\", ncol(panel), \"\\n\")\n\nColumns: 9 \n\ncat(\"\\nColumn names:\", paste(names(panel), collapse = \", \"), \"\\n\")\n\n\nColumn names: id, year, g, unit_fe, time_fe, treated, tau_gt, y, first_treat \n\ncat(\"\\nTreatment status:\\n\")\n\n\nTreatment status:\n\ncat(\"Treated observations:\", format(sum(panel$treated), big.mark = \",\"), \"\\n\")\n\nTreated observations: 3,802,782 \n\ncat(\"Control observations:\", format(sum(!panel$treated), big.mark = \",\"), \"\\n\")\n\nControl observations: 6,197,218 \n\ncat(\"\\nMemory usage:\", format(object.size(panel), units = \"MB\"), \"\\n\")\n\n\nMemory usage: 610.4 Mb",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "r_analysis.html",
    "href": "r_analysis.html",
    "title": "3  R Package Analysis",
    "section": "",
    "text": "4 R Packages for Staggered DiD\nThis chapter benchmarks four major R packages for difference-in-differences with staggered treatment adoption.\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the simulated data\npanel &lt;- readRDS(\"sim_data.rds\")\n\ncat(\"Data loaded:\\n\")\n\nData loaded:\n\ncat(\"Observations:\", format(nrow(panel), big.mark = \",\"), \"\\n\")\n\nObservations: 10,000,000 \n\ncat(\"Units:\", format(length(unique(panel$id)), big.mark = \",\"), \"\\n\")\n\nUnits: 1,000,000 \n\n# Calculate true overall ATT for comparison\ntrue_overall_att &lt;- mean(panel$tau_gt[panel$treated])\ncat(\"True overall ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue overall ATT: 1.5869 \n\n# Initialize results storage\nresults &lt;- list()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#callaway-santanna-did-package",
    "href": "r_analysis.html#callaway-santanna-did-package",
    "title": "3  R Package Analysis",
    "section": "4.1 1. Callaway & Sant’Anna (did package)",
    "text": "4.1 1. Callaway & Sant’Anna (did package)\nThe did package implements the method from Callaway & Sant’Anna (2021), estimating group-time average treatment effects and aggregating them.\nMethod: Doubly-robust estimation combining outcome regression and propensity score weighting.\n\nlibrary(did)\n\ncat(\"Running did::att_gt()...\\n\")\n\nRunning did::att_gt()...\n\ncat(\"This may take several minutes with 1M units.\\n\\n\")\n\nThis may take several minutes with 1M units.\n\n# Time the estimation\nstart_time &lt;- Sys.time()\n\n# Estimate group-time ATTs\n# Using subset for large data (full data may require significant memory)\nout_cs &lt;- att_gt(\n  yname = \"y\",\n  gname = \"first_treat\",\n  idname = \"id\",\n  tname = \"year\",\n  data = panel,\n  control_group = \"nevertreated\",\n  est_method = \"dr\",  # doubly robust\n  base_period = \"universal\"\n)\n\ncs_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"\\nExecution time:\", round(cs_time, 2), \"seconds\\n\")\n\n\nExecution time: 31.49 seconds\n\n# Aggregate to overall ATT\nagg_cs &lt;- aggte(out_cs, type = \"simple\")\ncat(\"\\nOverall ATT estimate:\", round(agg_cs$overall.att, 4), \"\\n\")\n\n\nOverall ATT estimate: 1.5846 \n\ncat(\"Standard error:\", round(agg_cs$overall.se, 4), \"\\n\")\n\nStandard error: 0.0019 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\n# Event study aggregation\nes_cs &lt;- aggte(out_cs, type = \"dynamic\")\n\nresults$did &lt;- list(\n  package = \"did\",\n  method = \"Callaway & Sant'Anna\",\n  time = cs_time,\n  att = agg_cs$overall.att,\n  se = agg_cs$overall.se,\n  event_study = es_cs\n)\n\n\n# Plot event study\nggdid(es_cs) +\n  labs(title = \"did package: Event Study\",\n       subtitle = paste(\"Execution time:\", round(cs_time, 1), \"seconds\"))\n\n\n\n\nEvent study: Callaway & Sant’Anna (did package)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#de-chaisemartin-dhaultfoeuille-didmultiplegtdyn",
    "href": "r_analysis.html#de-chaisemartin-dhaultfoeuille-didmultiplegtdyn",
    "title": "3  R Package Analysis",
    "section": "4.2 2. de Chaisemartin & D’Haultfoeuille (DIDmultiplegtDYN)",
    "text": "4.2 2. de Chaisemartin & D’Haultfoeuille (DIDmultiplegtDYN)\nThe DIDmultiplegtDYN package implements the method from de Chaisemartin & D’Haultfoeuille, robust to heterogeneous treatment effects.\nMethod: Comparing switchers to non-switchers at each period.\n\n# Check if package is available\nif (requireNamespace(\"DIDmultiplegtDYN\", quietly = TRUE)) {\n  library(DIDmultiplegtDYN)\n  library(dplyr)  # for pipe operator\n\n  cat(\"Running DIDmultiplegtDYN::did_multiplegt_dyn()...\\n\")\n  cat(\"This estimator can be slow with large datasets.\\n\\n\")\n\n  # Prepare data - DIDmultiplegtDYN needs treatment as binary\n  panel_dcdh &lt;- panel %&gt;%\n    mutate(D = as.integer(treated)) %&gt;%\n    select(id, year, y, D)\n\n  start_time &lt;- Sys.time()\n\n  # Note: For very large datasets, this may require sampling\n  # The package is computationally intensive\n  tryCatch({\n    out_dcdh &lt;- did_multiplegt_dyn(\n      df = panel_dcdh,\n      outcome = \"y\",\n      group = \"id\",\n      time = \"year\",\n      treatment = \"D\",\n      effects = 5,  # Number of event-study effects\n      placebo = 3,  # Number of pre-treatment placebos\n      cluster = \"id\",\n      graph_off = TRUE\n    )\n\n    dcdh_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"\\nExecution time:\", round(dcdh_time, 2), \"seconds\\n\")\n    print(summary(out_dcdh))\n\n    # Extract ATT from results$ATE\n    dcdh_att &lt;- out_dcdh$results$ATE\n\n    cat(\"\\nAverage Total Effect (ATT):\", round(dcdh_att, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didmultiplegt &lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille\",\n      time = dcdh_time,\n      att = dcdh_att,\n      output = out_dcdh\n    )\n  }, error = function(e) {\n    cat(\"Error running DIDmultiplegtDYN:\", e$message, \"\\n\")\n    cat(\"This package may have memory constraints with 1M observations.\\n\")\n    cat(\"Consider using a subsample for this estimator.\\n\")\n\n    results$didmultiplegt &lt;&lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille\",\n      time = NA,\n      error = e$message\n    )\n  })\n\n} else {\n  cat(\"Package DIDmultiplegtDYN not installed.\\n\")\n  cat(\"Install with: install.packages('DIDmultiplegtDYN')\\n\")\n\n  results$didmultiplegt &lt;- list(\n    package = \"DIDmultiplegtDYN\",\n    method = \"de Chaisemartin & D'Haultfoeuille\",\n    time = NA,\n    error = \"Package not installed\"\n  )\n}\n\nRunning DIDmultiplegtDYN::did_multiplegt_dyn()...\nThis estimator can be slow with large datasets.\n\nError running DIDmultiplegtDYN: vector memory limit of 36.0 Gb reached, see mem.maxVSize() \nThis package may have memory constraints with 1M observations.\nConsider using a subsample for this estimator.\n\n\n\n4.2.1 Alternative: Using a subsample\nDue to computational constraints, we may need to use a subsample:\n\nif (requireNamespace(\"DIDmultiplegtDYN\", quietly = TRUE) &&\n    (is.null(results$didmultiplegt$time) || is.na(results$didmultiplegt$time))) {\n\n  library(DIDmultiplegtDYN)\n  library(dplyr)  # for pipe operator\n\n  cat(\"Running DIDmultiplegtDYN on 10% subsample...\\n\")\n\n  # Sample 10% of units\n  set.seed(123)\n  sample_ids &lt;- sample(unique(panel$id), size = 100000)\n  panel_sample &lt;- panel %&gt;%\n    filter(id %in% sample_ids) %&gt;%\n    mutate(D = as.integer(treated)) %&gt;%\n    select(id, year, y, D)\n\n  cat(\"Subsample size:\", format(nrow(panel_sample), big.mark = \",\"), \"\\n\\n\")\n\n  start_time &lt;- Sys.time()\n\n  tryCatch({\n    out_dcdh_sample &lt;- did_multiplegt_dyn(\n      df = panel_sample,\n      outcome = \"y\",\n      group = \"id\",\n      time = \"year\",\n      treatment = \"D\",\n      effects = 5,\n      placebo = 3,\n      cluster = \"id\",\n      graph_off = TRUE\n    )\n\n    dcdh_time_sample &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"\\nExecution time (10% sample):\", round(dcdh_time_sample, 2), \"seconds\\n\")\n    cat(\"Estimated full data time:\", round(dcdh_time_sample * 10, 2), \"seconds\\n\")\n    print(summary(out_dcdh_sample))\n\n    # Extract ATT from results$ATE\n    dcdh_att_sample &lt;- out_dcdh_sample$results$ATE\n\n    cat(\"\\nAverage Total Effect (ATT):\", round(dcdh_att_sample, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didmultiplegt_sample &lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille (10% sample)\",\n      time = dcdh_time_sample,\n      att = dcdh_att_sample,\n      estimated_full_time = dcdh_time_sample * 10,\n      output = out_dcdh_sample\n    )\n  }, error = function(e) {\n    cat(\"Error:\", e$message, \"\\n\")\n  })\n}\n\nRunning DIDmultiplegtDYN on 10% subsample...\nSubsample size: 1,000,000 \n\n\nExecution time (10% sample): 76.09 seconds\nEstimated full data time: 760.9 seconds\n\n----------------------------------------------------------------------\n       Estimation of treatment effects: Event-study effects\n----------------------------------------------------------------------\n             Estimate SE      LB CI   UB CI   N       Switchers\nEffect_1     1.23956  0.00624 1.22734 1.25179 279,385 70,318   \nEffect_2     1.35506  0.00621 1.34288 1.36723 279,385 70,318   \nEffect_3     1.49808  0.00708 1.48420 1.51196 189,344 60,368   \nEffect_4     1.59630  0.00709 1.58241 1.61020 189,344 60,368   \nEffect_5     1.79333  0.00890 1.77589 1.81077 109,652 40,338   \n\nTest of joint nullity of the effects : p-value = 0.0000\n----------------------------------------------------------------------\n    Average cumulative (total) effect per treatment unit\n----------------------------------------------------------------------\n Estimate        SE     LB CI     UB CI         N Switchers \n  1.46362   0.00528   1.45327   1.47398   719,844   301,710 \nAverage number of time periods over which a treatment effect is accumulated: 2.7683\n\n----------------------------------------------------------------------\n     Testing the parallel trends and no anticipation assumptions\n----------------------------------------------------------------------\n             Estimate SE      LB CI    UB CI    N       Switchers\nPlacebo_1    -0.00871 0.00624 -0.02094 0.00353  279,385 70,318   \nPlacebo_2    -0.01505 0.00749 -0.02974 -0.00037 179,385 50,409   \nPlacebo_3    -0.00406 0.00887 -0.02144 0.01333  109,773 40,459   \n\nTest of joint nullity of the placebos : p-value = 0.2133\n\n\nThe development of this package was funded by the European Union.\nERC REALLYCREDIBLE - GA N. 101043899\nNULL\n\nAverage Total Effect (ATT): 1.4636 0.0053 1.4533 1.474 719844 301710 719844 301710 \nTrue ATT: 1.5869",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#sun-abraham-fixestsunab",
    "href": "r_analysis.html#sun-abraham-fixestsunab",
    "title": "3  R Package Analysis",
    "section": "4.3 3. Sun & Abraham (fixest::sunab)",
    "text": "4.3 3. Sun & Abraham (fixest::sunab)\nThe fixest package provides the interaction-weighted estimator from Sun & Abraham (2021) through the sunab() function.\nMethod: Cohort-specific coefficients with clean controls.\n\nlibrary(fixest)\n\ncat(\"Running fixest::feols() with sunab()...\\n\\n\")\n\nRunning fixest::feols() with sunab()...\n\n# Create cohort variable for sunab\npanel_sa &lt;- panel %&gt;%\n  mutate(\n    cohort = ifelse(first_treat == 0, Inf, first_treat),  # Never-treated = Inf\n    rel_time = ifelse(first_treat == 0, -1000, year - first_treat)  # Relative time\n  )\n\nstart_time &lt;- Sys.time()\n\n# Sun & Abraham estimation using sunab\nout_sa &lt;- feols(\n  y ~ sunab(cohort, year) | id + year,\n  data = panel_sa,\n  cluster = ~id\n)\n\nsa_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"Execution time:\", round(sa_time, 2), \"seconds\\n\\n\")\n\nExecution time: 27.16 seconds\n\n# Summary\nsummary(out_sa, agg = \"ATT\")\n\nOLS estimation, Dep. Var.: y\nObservations: 10,000,000\nFixed-effects: id: 1,000,000,  year: 10\nStandard-errors: Clustered (id) \n    Estimate Std. Error t value  Pr(&gt;|t|)    \nATT  1.58456   0.001775 892.853 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.948956     Adj. R2: 0.636526\n                 Within R2: 0.14887 \n\n# Get aggregated ATT - fixest aggregate returns different structure\nagg_sa &lt;- summary(out_sa, agg = \"ATT\")\n\n# Extract ATT from the aggregated coefficients\nsa_coefs &lt;- coef(agg_sa)\nsa_ses &lt;- se(agg_sa)\n\n# Filter to post-treatment effects (event time &gt;= 0) and compute mean\npost_idx &lt;- grep(\"^year::\", names(sa_coefs))\nif (length(post_idx) &gt; 0) {\n  # Get event times from coefficient names\n  event_times &lt;- as.numeric(gsub(\"year::\", \"\", names(sa_coefs)[post_idx]))\n  post_treatment &lt;- event_times &gt;= 0\n  sa_att &lt;- mean(sa_coefs[post_idx][post_treatment], na.rm = TRUE)\n  sa_se &lt;- mean(sa_ses[post_idx][post_treatment], na.rm = TRUE)\n} else {\n  sa_att &lt;- mean(sa_coefs, na.rm = TRUE)\n  sa_se &lt;- mean(sa_ses, na.rm = TRUE)\n}\n\ncat(\"\\nOverall ATT estimate (post-treatment avg):\", round(sa_att, 4), \"\\n\")\n\n\nOverall ATT estimate (post-treatment avg): 1.5846 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\nresults$sunab &lt;- list(\n  package = \"fixest\",\n  method = \"Sun & Abraham (sunab)\",\n  time = sa_time,\n  att = sa_att,\n  se = sa_se,\n  model = out_sa\n)\n\n\n# Event study plot\niplot(out_sa,\n      main = paste(\"fixest::sunab Event Study\\nExecution time:\",\n                   round(sa_time, 1), \"seconds\"))\n\n\n\n\nEvent study: Sun & Abraham (fixest::sunab)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#imputation-estimator-didimputation",
    "href": "r_analysis.html#imputation-estimator-didimputation",
    "title": "3  R Package Analysis",
    "section": "4.4 4. Imputation Estimator (didimputation)",
    "text": "4.4 4. Imputation Estimator (didimputation)\nThe didimputation package implements the imputation approach from Borusyak, Jaravel & Spiess (2024).\nMethod: Impute counterfactual outcomes for treated units using never-treated/not-yet-treated.\n\nif (requireNamespace(\"didimputation\", quietly = TRUE)) {\n  library(didimputation)\n\n  cat(\"Running didimputation::did_imputation()...\\n\\n\")\n\n  # Prepare data\n  panel_imp &lt;- panel %&gt;%\n    mutate(\n      first_treat = ifelse(first_treat == 0, NA_integer_, first_treat),\n      rel_time = ifelse(is.na(first_treat), NA_integer_, year - first_treat)\n    )\n\n  start_time &lt;- Sys.time()\n\n  tryCatch({\n    out_imp &lt;- did_imputation(\n      data = panel_imp,\n      yname = \"y\",\n      gname = \"first_treat\",\n      tname = \"year\",\n      idname = \"id\",\n      first_stage = ~ 0 | id + year,\n      horizon = TRUE,\n      pretrends = TRUE\n    )\n\n    imp_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"Execution time:\", round(imp_time, 2), \"seconds\\n\\n\")\n    print(out_imp)\n\n    # Extract overall ATT\n    att_rows &lt;- out_imp$term &gt;= 0\n    overall_att_imp &lt;- mean(out_imp$estimate[att_rows])\n\n    cat(\"\\nOverall ATT (post-treatment avg):\", round(overall_att_imp, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didimputation &lt;- list(\n      package = \"didimputation\",\n      method = \"Borusyak, Jaravel & Spiess\",\n      time = imp_time,\n      att = overall_att_imp,\n      output = out_imp\n    )\n  }, error = function(e) {\n    cat(\"Error running didimputation:\", e$message, \"\\n\")\n    results$didimputation &lt;&lt;- list(\n      package = \"didimputation\",\n      method = \"Borusyak, Jaravel & Spiess\",\n      time = NA,\n      error = e$message\n    )\n  })\n\n} else {\n  cat(\"Package didimputation not installed.\\n\")\n  cat(\"Install with: install.packages('didimputation')\\n\")\n\n  results$didimputation &lt;- list(\n    package = \"didimputation\",\n    method = \"Borusyak, Jaravel & Spiess\",\n    time = NA,\n    error = \"Package not installed\"\n  )\n}\n\nRunning didimputation::did_imputation()...\n\n\nError running didimputation: LU factorization of .gCMatrix failed: out of memory or near-singular \n\n\n\nif (!is.null(results$didimputation$output)) {\n  out_imp &lt;- results$didimputation$output\n  imp_time &lt;- results$didimputation$time\n\n  ggplot(out_imp, aes(x = term, y = estimate)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_vline(xintercept = -0.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      x = \"Event Time\",\n      y = \"Estimate\",\n      title = \"didimputation Event Study\",\n      subtitle = paste(\"Execution time:\", round(imp_time, 1), \"seconds\")\n    ) +\n    theme_minimal()\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#traditional-twfe-biased-baseline",
    "href": "r_analysis.html#traditional-twfe-biased-baseline",
    "title": "3  R Package Analysis",
    "section": "4.5 5. Traditional TWFE (Biased Baseline)",
    "text": "4.5 5. Traditional TWFE (Biased Baseline)\nFor comparison, let’s also run traditional two-way fixed effects, which is known to be biased with heterogeneous effects:\n\nlibrary(fixest)\n\ncat(\"Running traditional TWFE for comparison...\\n\\n\")\n\nRunning traditional TWFE for comparison...\n\nstart_time &lt;- Sys.time()\n\nout_twfe &lt;- feols(\n  y ~ treated | id + year,\n  data = panel,\n  cluster = ~id\n)\n\ntwfe_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"Execution time:\", round(twfe_time, 2), \"seconds\\n\\n\")\n\nExecution time: 1.1 seconds\n\nsummary(out_twfe)\n\nOLS estimation, Dep. Var.: y\nObservations: 10,000,000\nFixed-effects: id: 1,000,000,  year: 10\nStandard-errors: Clustered (id) \n            Estimate Std. Error t value  Pr(&gt;|t|)    \ntreatedTRUE  1.32861   0.001155 1150.25 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.958966     Adj. R2: 0.628819\n                 Within R2: 0.130818\n\ncat(\"\\nTWFE ATT estimate:\", round(coef(out_twfe), 4), \"\\n\")\n\n\nTWFE ATT estimate: 1.3286 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\ncat(\"Bias:\", round(coef(out_twfe) - true_overall_att, 4), \"\\n\")\n\nBias: -0.2583 \n\nresults$twfe &lt;- list(\n  package = \"fixest\",\n  method = \"Traditional TWFE (biased)\",\n  time = twfe_time,\n  att = coef(out_twfe),\n  se = se(out_twfe)\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#r-results-summary",
    "href": "r_analysis.html#r-results-summary",
    "title": "3  R Package Analysis",
    "section": "4.6 R Results Summary",
    "text": "4.6 R Results Summary\n\n# Create summary table\nsummary_df &lt;- data.frame(\n  Package = character(),\n  Method = character(),\n  Time_seconds = numeric(),\n  ATT_estimate = numeric(),\n  True_ATT = numeric(),\n  Bias = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (name in names(results)) {\n  r &lt;- results[[name]]\n  att &lt;- if (!is.null(r$att)) r$att else NA\n  summary_df &lt;- rbind(summary_df, data.frame(\n    Package = r$package,\n    Method = r$method,\n    Time_seconds = ifelse(is.null(r$time), NA, r$time),\n    ATT_estimate = att,\n    True_ATT = true_overall_att,\n    Bias = ifelse(is.na(att), NA, att - true_overall_att),\n    stringsAsFactors = FALSE\n  ))\n}\n\nknitr::kable(summary_df, digits = 4,\n             caption = \"R Package Comparison Summary\")\n\n\nR Package Comparison Summary\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nTime_seconds\nATT_estimate\nTrue_ATT\nBias\n\n\n\n\n1\ndid\nCallaway & Sant’Anna\n30.0508\n1.5846\n1.5869\n-0.0023\n\n\n2\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille\nNA\nNA\n1.5869\nNA\n\n\n3\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille (10% sample)\n71.3493\nNA\n1.5869\nNA\n\n\n4\nfixest\nSun & Abraham (sunab)\n25.4929\n1.5846\n1.5869\n-0.0023\n\n\n5\ndidimputation\nBorusyak, Jaravel & Spiess\nNA\nNA\n1.5869\nNA\n\n\ntreatedTRUE\nfixest\nTraditional TWFE (biased)\n0.9361\n1.3286\n1.5869\n-0.2583\n\n\n\n\n# Save results for comparison chapter\nsaveRDS(results, \"r_results.rds\")\n\n\n# Filter to valid times\ntiming_df &lt;- summary_df %&gt;%\n  filter(!is.na(Time_seconds))\n\nggplot(timing_df, aes(x = reorder(Package, Time_seconds), y = Time_seconds)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    x = \"Package\",\n    y = \"Execution Time (seconds)\",\n    title = \"R Package Execution Times\",\n    subtitle = paste(\"Dataset:\", format(nrow(panel), big.mark = \",\"), \"observations\")\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = round(Time_seconds, 1)), hjust = -0.1)\n\n\n\n\nExecution time comparison (R packages)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html",
    "href": "python_analysis.html",
    "title": "4  Python Package Analysis",
    "section": "",
    "text": "5 Python Packages for Staggered DiD\nThis chapter benchmarks Python packages for difference-in-differences with staggered treatment. Python has fewer native implementations compared to R, but several community packages exist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#package-availability-summary",
    "href": "python_analysis.html#package-availability-summary",
    "title": "4  Python Package Analysis",
    "section": "5.1 Package Availability Summary",
    "text": "5.1 Package Availability Summary\n\n\n\n\n\n\n\n\n\nMethod\nR Package\nPython Package\nStatus\n\n\n\n\nCallaway & Sant’Anna\ndid\ncsdid, diff_diff\nAvailable\n\n\nde Chaisemartin & D’Haultfoeuille\nDIDmultiplegtDYN\ndid_multiplegt_dyn\nAvailable\n\n\nSun & Abraham\nfixest::sunab\npyfixest\nAvailable (partial)\n\n\nBorusyak, Jaravel & Spiess\ndidimputation\nNone\nNot available in Python\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the data\ndf = pd.read_csv(\"sim_data.csv\")\n\nprint(f\"Data loaded:\")\nprint(f\"Observations: {len(df):,}\")\nprint(f\"Units: {df['id'].nunique():,}\")\nprint(f\"Periods: {df['year'].nunique()}\")\n\n# Calculate true ATT (from treated observations' true effects)\n# We need to regenerate true effects since CSV doesn't have them\ntreat_cohorts = [0, 2012, 2014, 2016, 2018]\ncohort_effects = {0: 0, 2012: 0.5, 2014: 0.3, 2016: 0.1, 2018: 0.0}\ntau_0 = 1.0\ngamma = 0.1\n\ndf['tau_gt'] = 0.0\nmask = df['treated'] == True\ndf.loc[mask, 'tau_gt'] = (\n    tau_0 +\n    df.loc[mask, 'first_treat'].map(cohort_effects) +\n    gamma * (df.loc[mask, 'year'] - df.loc[mask, 'first_treat'])\n)\n\ntrue_overall_att = df.loc[df['treated'], 'tau_gt'].mean()\nprint(f\"\\nTrue overall ATT: {true_overall_att:.4f}\")\n\n# Store results\nresults = {}\n\nData loaded:\nObservations: 100,000\nUnits: 10,000\nPeriods: 10\n\nTrue overall ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#callaway-santanna-csdid",
    "href": "python_analysis.html#callaway-santanna-csdid",
    "title": "4  Python Package Analysis",
    "section": "5.2 1. Callaway & Sant’Anna (csdid)",
    "text": "5.2 1. Callaway & Sant’Anna (csdid)\nThe csdid package is a Python port of the original R did package.\n\ntry:\n    from csdid.att_gt import ATTgt\n\n    print(\"Running csdid ATTgt()...\")\n    print(\"This may take several minutes with 1M units.\\n\")\n\n    start_time = time.perf_counter()\n\n    att_gt = ATTgt(\n        yname='y',\n        gname='first_treat',\n        idname='id',\n        tname='year',\n        data=df\n    )\n\n    out_csdid = att_gt.fit(est_method='dr')  # doubly robust\n\n    csdid_time = time.perf_counter() - start_time\n\n    print(f\"\\nExecution time: {csdid_time:.2f} seconds\")\n\n    # Aggregate to dynamic effects\n    agg_dynamic = out_csdid.aggte(typec='dynamic', na_rm=True)\n\n    # Extract overall ATT from the aggregation\n    csdid_att = agg_dynamic.summ_attgt().atte['overall_att']\n\n    print(f\"\\nEstimated ATT: {csdid_att:.4f}\" if csdid_att else \"ATT not extracted\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': csdid_time,\n        'att': csdid_att,\n        'output': out_csdid\n    }\n\nexcept ImportError as e:\n    print(\"Package csdid not installed.\")\n    print(\"Install with: pip install csdid\")\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running csdid: {e}\")\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning csdid ATTgt()...\nThis may take several minutes with 1M units.\n\n\nExecution time: 0.61 seconds\n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:\n   ATT Std. Error [95.0%  Conf. Int.]  \n1.7041     0.0201 1.6647       1.7435 *\n\n\nDynamic Effects:\n    Event time  Estimate  Std. Error  [95.0% Simult.   Conf. Band   \n0           -7    0.0760      0.0493          -0.0207      0.1727   \n1           -6    0.0036      0.0497          -0.0939      0.1010   \n2           -5    0.0034      0.0327          -0.0607      0.0674   \n3           -4   -0.0378      0.0318          -0.1002      0.0245   \n4           -3    0.0141      0.0250          -0.0350      0.0631   \n5           -2   -0.0178      0.0240          -0.0648      0.0292   \n6           -1    0.0108      0.0218          -0.0318      0.0534   \n7            0    1.2330      0.0225           1.1889      1.2771  *\n8            1    1.3592      0.0175           1.3250      1.3934  *\n9            2    1.5132      0.0258           1.4627      1.5637  *\n10           3    1.5721      0.0212           1.5305      1.6137  *\n11           4    1.7740      0.0300           1.7152      1.8329  *\n12           5    1.8917      0.0295           1.8339      1.9496  *\n13           6    2.1220      0.0402           2.0432      2.2008  *\n14           7    2.1674      0.0397           2.0896      2.2453  *\n---\nSignif. codes: `*' confidence band does not cover 0\nControl Group:  Never Treated , \nAnticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\n\nEstimated ATT: 1.7041\nTrue ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#alternative-cs-implementation-diff_diff",
    "href": "python_analysis.html#alternative-cs-implementation-diff_diff",
    "title": "4  Python Package Analysis",
    "section": "5.3 2. Alternative CS Implementation (diff_diff)",
    "text": "5.3 2. Alternative CS Implementation (diff_diff)\nThe diff_diff package provides another implementation of Callaway & Sant’Anna.\n\ntry:\n    from diff_diff import CallawaySantAnna\n\n    print(\"Running diff_diff CallawaySantAnna()...\")\n    print()\n\n    cs = CallawaySantAnna()\n\n    start_time = time.perf_counter()\n\n    cs_results = cs.fit(\n        df,\n        outcome='y',\n        unit='id',\n        time='year',\n        first_treat='first_treat',\n        aggregate='event_study'\n    )\n\n    diff_diff_time = time.perf_counter() - start_time\n\n    print(f\"Execution time: {diff_diff_time:.2f} seconds\")\n\n    # Extract ATT from event study\n    if hasattr(cs_results, 'event_study_effects'):\n        post_effects = [v['effect'] for k, v in cs_results.event_study_effects.items()\n                       if k &gt;= 0]\n        diff_diff_att = np.mean(post_effects) if post_effects else None\n    else:\n        diff_diff_att = None\n\n    print(f\"\\nEstimated ATT (post-treatment avg): {diff_diff_att:.4f}\" if diff_diff_att else \"\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': diff_diff_time,\n        'att': diff_diff_att,\n        'output': cs_results\n    }\n\nexcept ImportError as e:\n    print(\"Package diff_diff not installed.\")\n    print(\"Install with: pip install diff-diff\")\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running diff_diff: {e}\")\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning diff_diff CallawaySantAnna()...\n\nExecution time: 0.05 seconds\n\nEstimated ATT (post-treatment avg): 1.7041\nTrue ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#sun-abraham-via-pyfixest",
    "href": "python_analysis.html#sun-abraham-via-pyfixest",
    "title": "4  Python Package Analysis",
    "section": "5.4 3. Sun & Abraham via pyfixest",
    "text": "5.4 3. Sun & Abraham via pyfixest\nThe pyfixest package is a Python port of the R fixest package and includes support for Sun & Abraham estimation.\n\ntry:\n    import pyfixest as pf\n\n    print(\"Running pyfixest for Sun & Abraham...\")\n    print()\n\n    # Prepare data for pyfixest\n    df_pf = df.copy()\n    df_pf['cohort'] = df_pf['first_treat'].replace(0, np.inf)  # Never-treated = Inf\n\n    start_time = time.perf_counter()\n\n    # Sun & Abraham estimation\n    # pyfixest uses i() for interaction-weighted estimator\n    try:\n        # Try sunab-style estimation\n        out_pf = pf.feols(\n            \"y ~ sunab(cohort, year) | id + year\",\n            data=df_pf,\n            vcov={'CRV1': 'id'}\n        )\n        pf_time = time.perf_counter() - start_time\n\n        print(f\"Execution time: {pf_time:.2f} seconds\")\n        print(out_pf.summary())\n\n        # Extract ATT\n        pf_att = out_pf.coef().mean()  # Average of event study coefficients\n\n        results['pyfixest'] = {\n            'package': 'pyfixest',\n            'method': 'Sun & Abraham',\n            'time': pf_time,\n            'att': pf_att,\n            'output': out_pf\n        }\n\n    except Exception as e:\n        # Fallback to simple TWFE\n        print(f\"sunab not available in pyfixest, running TWFE: {e}\")\n\n        out_pf = pf.feols(\n            \"y ~ treated | id + year\",\n            data=df_pf,\n            vcov={'CRV1': 'id'}\n        )\n        pf_time = time.perf_counter() - start_time\n\n        print(f\"\\nExecution time (TWFE): {pf_time:.2f} seconds\")\n        print(out_pf.summary())\n\n        results['pyfixest'] = {\n            'package': 'pyfixest',\n            'method': 'TWFE (sunab not available)',\n            'time': pf_time,\n            'att': out_pf.coef()['treated'],\n            'output': out_pf\n        }\n\nexcept ImportError as e:\n    print(\"Package pyfixest not installed.\")\n    print(\"Install with: pip install pyfixest\")\n    results['pyfixest'] = {\n        'package': 'pyfixest',\n        'method': 'Sun & Abraham',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running pyfixest: {e}\")\n    results['pyfixest'] = {\n        'package': 'pyfixest',\n        'method': 'Sun & Abraham',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning pyfixest for Sun & Abraham...\n\nsunab not available in pyfixest, running TWFE: Unable to evaluate factor `sunab(cohort, year)`. [NameError: name 'sunab' is not defined]\n\n\n\nExecution time (TWFE): 5.17 seconds\n###\n\nEstimation:  OLS\nDep. var.: y, Fixed effects: id+year\nInference:  CRV1\nObservations:  100000\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| treated       |      1.324 |        0.012 |   114.788 |      0.000 |  1.302 |   1.347 |\n---\nRMSE: 0.958 R2: 0.668 R2 Within: 0.13 \nNone",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#de-chaisemartin-dhaultfoeuille-did_multiplegt_dyn",
    "href": "python_analysis.html#de-chaisemartin-dhaultfoeuille-did_multiplegt_dyn",
    "title": "4  Python Package Analysis",
    "section": "5.5 4. de Chaisemartin & D’Haultfoeuille (did_multiplegt_dyn)",
    "text": "5.5 4. de Chaisemartin & D’Haultfoeuille (did_multiplegt_dyn)\nThe did_multiplegt_dyn package is a Python port of the original Stata/R command by de Chaisemartin & D’Haultfoeuille.\nMethod: Compares switchers to non-switchers at each period, robust to heterogeneous treatment effects.\n\nimport polars as pl\nfrom did_multiplegt_dyn import DidMultiplegtDyn\n\nprint(\"Running DidMultiplegtDyn()...\")\nprint(\"Note: This estimator can be computationally intensive.\\n\")\n\n# Convert pandas DataFrame to polars and prepare data\ndf_dcdh = pl.from_pandas(df)\ndf_dcdh = df_dcdh.with_columns([\n    pl.col('treated').cast(pl.Int32).alias('D')\n])\nstart_time = time.perf_counter()\n\ntry:\n    # Create model instance\n    model_dcdh = DidMultiplegtDyn(\n        df=df_dcdh,\n        outcome='y',\n        group='id',\n        time='year',\n        treatment='D',\n        effects=5,\n        placebo=3,\n        cluster='id'\n    )\n\n    # Fit the model\n    model_dcdh.fit()\n\n    dcdh_time = time.perf_counter() - start_time\n\n    # Get summary table\n    dcdh_summary = model_dcdh.summary()\n\n    print(f\"\\nExecution time (10% sample): {dcdh_time:.2f} seconds\")\n    print(f\"Estimated full data time: ~{dcdh_time * 10:.0f} seconds\")\n    print()\n    print(dcdh_summary)\n\n    # Extract Average_Total_Effect from summary table\n    dcdh_att = model_dcdh.result['did_multiplegt_dyn']['ATE']['Estimate'].values[0]\n\n    results['did_multiplegt_dyn'] = {\n        'package': 'did_multiplegt_dyn',\n        'method': 'de Chaisemartin & D\\'Haultfoeuille (10% sample)',\n        'time': dcdh_time,\n        'estimated_full_time': dcdh_time * 10,\n        'att': dcdh_att,\n        'output': dcdh_summary\n    }\n\nexcept Exception as e:\n    print(f\"Error during estimation: {e}\")\n    import traceback\n    traceback.print_exc()\n    results['did_multiplegt_dyn'] = {\n        'package': 'did_multiplegt_dyn',\n        'method': 'de Chaisemartin & D\\'Haultfoeuille',\n        'time': None,\n        'att': None,\n        'error': str(e)\n    }\n\nRunning DidMultiplegtDyn()...\nNote: This estimator can be computationally intensive.\n\n               Block  Estimate       SE     LB CI    UB CI       N  Switchers     N.w  Switchers.w\n            Effect_1  1.242962 0.019474  1.204793 1.281131 28044.0     7009.0 28044.0       7009.0\n            Effect_2  1.361619 0.019661  1.323084 1.400153 28044.0     7009.0 28044.0       7009.0\n            Effect_3  1.524482 0.022447  1.480487 1.568477 19045.0     5992.0 19045.0       5992.0\n            Effect_4  1.574469 0.022517  1.530335 1.618602 19045.0     5992.0 19045.0       5992.0\n            Effect_5  1.781573 0.028124  1.726450 1.836696 10945.0     3946.0 10945.0       3946.0\nAverage_Total_Effect  1.464355 0.016578  1.431863 1.496847 72018.0    29948.0 72018.0      29948.0\n           Placebo_1 -0.003321 0.019518 -0.041577 0.034934 28044.0     7009.0 28044.0       7009.0\n           Placebo_2 -0.011834 0.023465 -0.057825 0.034156 18044.0     4991.0 18044.0       4991.0\n           Placebo_3 -0.018693 0.028306 -0.074171 0.036786 10973.0     3974.0 10973.0       3974.0\n\nExecution time (10% sample): 0.65 seconds\nEstimated full data time: ~6 seconds\n\n                  Block  Estimate        SE     LB CI     UB CI        N  \\\n0              Effect_1  1.242962  0.019474  1.204793  1.281131  28044.0   \n1              Effect_2  1.361619  0.019661  1.323084  1.400153  28044.0   \n2              Effect_3  1.524482  0.022447  1.480487  1.568477  19045.0   \n3              Effect_4  1.574469  0.022517  1.530335  1.618602  19045.0   \n4              Effect_5  1.781573  0.028124  1.726450  1.836696  10945.0   \n5  Average_Total_Effect  1.464355  0.016578  1.431863  1.496847  72018.0   \n6             Placebo_1 -0.003321  0.019518 -0.041577  0.034934  28044.0   \n7             Placebo_2 -0.011834  0.023465 -0.057825  0.034156  18044.0   \n8             Placebo_3 -0.018693  0.028306 -0.074171  0.036786  10973.0   \n\n   Switchers      N.w  Switchers.w  \n0     7009.0  28044.0       7009.0  \n1     7009.0  28044.0       7009.0  \n2     5992.0  19045.0       5992.0  \n3     5992.0  19045.0       5992.0  \n4     3946.0  10945.0       3946.0  \n5    29948.0  72018.0      29948.0  \n6     7009.0  28044.0       7009.0  \n7     4991.0  18044.0       4991.0  \n8     3974.0  10973.0       3974.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#borusyak-jaravel-spiess-imputation",
    "href": "python_analysis.html#borusyak-jaravel-spiess-imputation",
    "title": "4  Python Package Analysis",
    "section": "5.6 5. Borusyak, Jaravel & Spiess (Imputation)",
    "text": "5.6 5. Borusyak, Jaravel & Spiess (Imputation)\n\n\n\n\n\n\nNot Available in Python\n\n\n\nThere is no native Python package implementing the imputation estimator from Borusyak, Jaravel & Spiess. This method is only available in:\n\nR: didimputation package\nStata: did_imputation command\n\n\n\n\nprint(\"=\" * 60)\nprint(\"Borusyak, Jaravel & Spiess (did_imputation)\")\nprint(\"=\" * 60)\nprint()\nprint(\"STATUS: NOT AVAILABLE IN PYTHON\")\nprint()\nprint(\"This estimator is only available in:\")\nprint(\"  - R: didimputation package\")\nprint(\"  - Stata: did_imputation command\")\nprint()\n\nresults['didimputation'] = {\n    'package': 'N/A',\n    'method': 'Borusyak, Jaravel & Spiess',\n    'time': None,\n    'att': None,\n    'error': 'No Python implementation available'\n}\n\n============================================================\nBorusyak, Jaravel & Spiess (did_imputation)\n============================================================\n\nSTATUS: NOT AVAILABLE IN PYTHON\n\nThis estimator is only available in:\n  - R: didimputation package\n  - Stata: did_imputation command",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#traditional-twfe-biased-baseline",
    "href": "python_analysis.html#traditional-twfe-biased-baseline",
    "title": "4  Python Package Analysis",
    "section": "5.7 6. Traditional TWFE (Biased Baseline)",
    "text": "5.7 6. Traditional TWFE (Biased Baseline)\nFor comparison, let’s run traditional TWFE using linearmodels or statsmodels:\n\ntry:\n    from linearmodels.panel import PanelOLS\n\n    print(\"Running traditional TWFE with linearmodels...\")\n    print()\n\n    # Prepare panel data structure\n    df_panel = df.set_index(['id', 'year'])\n\n    start_time = time.perf_counter()\n\n    model = PanelOLS(\n        df_panel['y'],\n        df_panel[['treated']].astype(float),\n        entity_effects=True,\n        time_effects=True\n    )\n    out_twfe = model.fit(cov_type='clustered', cluster_entity=True)\n\n    twfe_time = time.perf_counter() - start_time\n\n    print(f\"Execution time: {twfe_time:.2f} seconds\")\n    print(out_twfe.summary.tables[1])\n\n    twfe_att = out_twfe.params['treated']\n    print(f\"\\nTWFE ATT estimate: {twfe_att:.4f}\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n    print(f\"Bias: {twfe_att - true_overall_att:.4f}\")\n\n    results['twfe_linearmodels'] = {\n        'package': 'linearmodels',\n        'method': 'Traditional TWFE (biased)',\n        'time': twfe_time,\n        'att': twfe_att,\n        'output': out_twfe\n    }\n\nexcept ImportError:\n    print(\"Package linearmodels not installed.\")\n    print(\"Install with: pip install linearmodels\")\n\n    # Fallback to statsmodels\n    try:\n        import statsmodels.api as sm\n        from statsmodels.regression.linear_model import OLS\n\n        print(\"\\nRunning TWFE with statsmodels (demeaned)...\")\n\n        # Demean for fixed effects\n        df_fe = df.copy()\n        df_fe['y_demeaned'] = df_fe.groupby('id')['y'].transform(lambda x: x - x.mean())\n        df_fe['y_demeaned'] = df_fe.groupby('year')['y_demeaned'].transform(lambda x: x - x.mean())\n        df_fe['treated_demeaned'] = df_fe.groupby('id')['treated'].transform(lambda x: x - x.mean())\n        df_fe['treated_demeaned'] = df_fe.groupby('year')['treated_demeaned'].transform(lambda x: x - x.mean())\n\n        start_time = time.perf_counter()\n        model = OLS(df_fe['y_demeaned'], df_fe['treated_demeaned']).fit()\n        twfe_time = time.perf_counter() - start_time\n\n        print(f\"Execution time: {twfe_time:.2f} seconds\")\n        print(f\"TWFE ATT: {model.params[0]:.4f}\")\n\n        results['twfe_statsmodels'] = {\n            'package': 'statsmodels',\n            'method': 'Traditional TWFE (biased)',\n            'time': twfe_time,\n            'att': model.params[0]\n        }\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nexcept Exception as e:\n    print(f\"Error running TWFE: {e}\")\n\nRunning traditional TWFE with linearmodels...\n\nExecution time: 0.09 seconds\n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\ntreated        1.3242     0.0122     108.90     0.0000      1.3003      1.3480\n==============================================================================\n\nTWFE ATT estimate: 1.3242\nTrue ATT: 1.5861\nBias: -0.2620",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#python-results-summary",
    "href": "python_analysis.html#python-results-summary",
    "title": "4  Python Package Analysis",
    "section": "5.8 Python Results Summary",
    "text": "5.8 Python Results Summary\n\nimport pandas as pd\n\n# Create summary table\nsummary_data = []\nfor name, r in results.items():\n    summary_data.append({\n        'Package': r.get('package', 'N/A'),\n        'Method': r.get('method', 'N/A'),\n        'Time (s)': r.get('time'),\n        'ATT': r.get('att'),\n        'True ATT': true_overall_att if r.get('time') else None,\n        'Bias': (r.get('att') - true_overall_att) if r.get('att') else None,\n        'Status': 'Error: ' + r.get('error', '') if r.get('error') else 'OK'\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PYTHON PACKAGE COMPARISON SUMMARY\")\nprint(\"=\" * 80)\nprint(summary_df.to_string(index=False))\n\n# Save for comparison chapter\nsummary_df.to_csv(\"python_results.csv\", index=False)\n\n\n================================================================================\nPYTHON PACKAGE COMPARISON SUMMARY\n================================================================================\n           Package                                         Method  Time (s)      ATT  True ATT      Bias                                    Status\n             csdid                           Callaway & Sant'Anna  0.607066 1.704085  1.586146  0.117939                                        OK\n         diff_diff                           Callaway & Sant'Anna  0.049023 1.704085  1.586146  0.117939                                        OK\n          pyfixest                     TWFE (sunab not available)  5.165600 1.324159  1.586146 -0.261987                                        OK\ndid_multiplegt_dyn de Chaisemartin & D'Haultfoeuille (10% sample)  0.645394 1.464355  1.586146 -0.121791                                        OK\n               N/A                     Borusyak, Jaravel & Spiess       NaN      NaN       NaN       NaN Error: No Python implementation available\n      linearmodels                      Traditional TWFE (biased)  0.087213 1.324159  1.586146 -0.261987                                        OK\n\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to packages with valid times\ntiming_data = [(r['package'], r['time']) for r in results.values()\n               if r.get('time') is not None]\n\nif timing_data:\n    packages, times = zip(*timing_data)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.barh(packages, times, color='steelblue')\n    ax.set_xlabel('Execution Time (seconds)')\n    ax.set_title(f'Python Package Execution Times\\nDataset: {len(df):,} observations')\n\n    # Add time labels\n    for bar, t in zip(bars, times):\n        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n                f'{t:.1f}s', va='center')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No timing data available to plot.\")\n\n\n\n\nExecution time comparison (Python packages)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#python-package-availability-notes",
    "href": "python_analysis.html#python-package-availability-notes",
    "title": "4  Python Package Analysis",
    "section": "5.9 Python Package Availability Notes",
    "text": "5.9 Python Package Availability Notes\n\n5.9.1 Available Packages\n\ncsdid: Full implementation of Callaway & Sant’Anna\n\nInstall: pip install csdid\nSupports doubly-robust estimation\nCan be slow with very large datasets\n\ndiff_diff: Alternative CS implementation\n\nInstall: pip install diff-diff\nGenerally faster than csdid\nGood for event study aggregation\n\npyfixest: Port of R’s fixest\n\nInstall: pip install pyfixest\nFast fixed effects estimation\nSun & Abraham support varies by version\n\ndid_multiplegt_dyn: de Chaisemartin & D’Haultfoeuille\n\nInstall: pip install did-multiplegt-dyn\nPython port of the R/Stata command\nComputationally intensive for large datasets\n\n\n\n\n5.9.2 Not Available in Python\n\n\n\n\n\n\nMissing Python Implementation\n\n\n\nThe following estimator does not have a native Python package:\nBorusyak, Jaravel & Spiess (didimputation) - Only available in R and Stata - No community Python port exists\nFor this method, users must either:\n\nUse R (recommended via Quarto or rpy2)\nUse Stata\nImplement the method from scratch",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "comparison.html",
    "href": "comparison.html",
    "title": "5  Summary and Comparison",
    "section": "",
    "text": "6 Overall Comparison\nThis chapter provides a comprehensive comparison of all DiD estimators tested across R and Python.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Load R results\nr_results &lt;- readRDS(\"r_results.rds\")\n\n# Load Python results\npython_results &lt;- read.csv(\"python_results.csv\")\n\n# True ATT\npanel &lt;- readRDS(\"sim_data.rds\")\ntrue_att &lt;- mean(panel$tau_gt[panel$treated])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#package-availability-summary",
    "href": "comparison.html#package-availability-summary",
    "title": "5  Summary and Comparison",
    "section": "6.1 Package Availability Summary",
    "text": "6.1 Package Availability Summary\n\navailability &lt;- data.frame(\n  Method = c(\n    \"Callaway & Sant'Anna\",\n    \"de Chaisemartin & D'Haultfoeuille\",\n    \"Sun & Abraham\",\n    \"Borusyak, Jaravel & Spiess (Imputation)\",\n    \"Traditional TWFE\"\n  ),\n  R_Package = c(\n    \"did\",\n    \"DIDmultiplegtDYN\",\n    \"fixest (sunab)\",\n    \"didimputation\",\n    \"fixest / lfe\"\n  ),\n  Python_Package = c(\n    \"csdid, diff_diff\",\n    \"did_multiplegt_dyn\",\n    \"pyfixest (partial)\",\n    \"NOT AVAILABLE\",\n    \"linearmodels, pyfixest\"\n  ),\n  Stata_Package = c(\n    \"csdid\",\n    \"did_multiplegt_dyn\",\n    \"eventstudyinteract\",\n    \"did_imputation\",\n    \"reghdfe\"\n  )\n)\n\nkable(availability, caption = \"Package Availability by Language\")\n\n\nPackage Availability by Language\n\n\n\n\n\n\n\n\nMethod\nR_Package\nPython_Package\nStata_Package\n\n\n\n\nCallaway & Sant’Anna\ndid\ncsdid, diff_diff\ncsdid\n\n\nde Chaisemartin & D’Haultfoeuille\nDIDmultiplegtDYN\ndid_multiplegt_dyn\ndid_multiplegt_dyn\n\n\nSun & Abraham\nfixest (sunab)\npyfixest (partial)\neventstudyinteract\n\n\nBorusyak, Jaravel & Spiess (Imputation)\ndidimputation\nNOT AVAILABLE\ndid_imputation\n\n\nTraditional TWFE\nfixest / lfe\nlinearmodels, pyfixest\nreghdfe",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#execution-time-comparison",
    "href": "comparison.html#execution-time-comparison",
    "title": "5  Summary and Comparison",
    "section": "6.2 Execution Time Comparison",
    "text": "6.2 Execution Time Comparison\n\n# Combine R results\nr_timing &lt;- data.frame(\n  Package = sapply(r_results, function(x) x$package),\n  Method = sapply(r_results, function(x) x$method),\n  Time = sapply(r_results, function(x) ifelse(is.null(x$time), NA, x$time)),\n  Language = \"R\"\n)\n\n# Add Python results\npython_timing &lt;- python_results %&gt;%\n  filter(!is.na(Time..s.)) %&gt;%\n  mutate(\n    Package = Package,\n    Method = Method,\n    Time = Time..s.,\n    Language = \"Python\"\n  ) %&gt;%\n  select(Package, Method, Time, Language)\n\n# Combine\nall_timing &lt;- bind_rows(r_timing, python_timing) %&gt;%\n  filter(!is.na(Time))\n\n# Plot\nggplot(all_timing, aes(x = reorder(paste(Package, Method, sep = \"\\n\"), Time),\n                       y = Time, fill = Language)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"Execution Time (seconds)\",\n    title = \"Execution Time Comparison\",\n    subtitle = paste(\"Dataset:\", format(nrow(panel), big.mark = \",\"), \"observations\")\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 8)) +\n  scale_fill_manual(values = c(\"R\" = \"steelblue\", \"Python\" = \"darkorange\")) +\n  geom_text(aes(label = sprintf(\"%.1fs\", Time)), hjust = -0.1, size = 3)\n\n\n\n\nExecution time comparison across all packages",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#estimation-accuracy",
    "href": "comparison.html#estimation-accuracy",
    "title": "5  Summary and Comparison",
    "section": "6.3 Estimation Accuracy",
    "text": "6.3 Estimation Accuracy\n\n# Extract ATT estimates\nr_att &lt;- data.frame(\n  Package = sapply(r_results, function(x) x$package),\n  Method = sapply(r_results, function(x) x$method),\n  ATT = sapply(r_results, function(x) {\n    if (!is.null(x$att)) x$att else NA\n  }),\n  Language = \"R\"\n)\n\npython_att &lt;- python_results %&gt;%\n  filter(!is.na(ATT)) %&gt;%\n  mutate(Language = \"Python\") %&gt;%\n  select(Package, Method, ATT, Language)\n\nall_att &lt;- bind_rows(r_att, python_att) %&gt;%\n  filter(!is.na(ATT)) %&gt;%\n  mutate(\n    Bias = ATT - true_att,\n    Pct_Bias = (ATT - true_att) / true_att * 100\n  )\n\nkable(all_att %&gt;% select(Language, Package, Method, ATT, Bias, Pct_Bias),\n      digits = 4,\n      col.names = c(\"Language\", \"Package\", \"Method\", \"ATT Estimate\", \"Bias\", \"% Bias\"),\n      caption = paste(\"Estimation Accuracy (True ATT =\", round(true_att, 4), \")\"))\n\n\nEstimation Accuracy (True ATT = 1.5869 )\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\nPackage\nMethod\nATT Estimate\nBias\n% Bias\n\n\n\n\ndid\nR\ndid\nCallaway & Sant’Anna\n1.5846\n-0.0023\n-0.1470\n\n\nsunab\nR\nfixest\nSun & Abraham (sunab)\n1.5846\n-0.0023\n-0.1470\n\n\ntwfe\nR\nfixest\nTraditional TWFE (biased)\n1.3286\n-0.2583\n-16.2756\n\n\n…4\nPython\ndiff_diff\nCallaway & Sant’Anna\n1.7122\n0.1253\n7.8989\n\n\n…5\nPython\npyfixest\nTWFE (sunab not available)\n1.3286\n-0.2583\n-16.2756\n\n\n…6\nPython\nlinearmodels\nTraditional TWFE (biased)\n1.3286\n-0.2583\n-16.2756\n\n\n\n\n\n\nggplot(all_att, aes(x = reorder(paste(Package, \"(\", Language, \")\", sep = \"\"), ATT),\n                    y = ATT, color = Language)) +\n  geom_point(size = 4) +\n  geom_hline(yintercept = true_att, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 0.5, y = true_att + 0.02, label = paste(\"True ATT =\", round(true_att, 3)),\n           hjust = 0, color = \"red\") +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"ATT Estimate\",\n    title = \"Estimated ATT vs True Value\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"R\" = \"steelblue\", \"Python\" = \"darkorange\"))\n\n\n\n\nATT estimates vs true value",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#key-findings",
    "href": "comparison.html#key-findings",
    "title": "5  Summary and Comparison",
    "section": "6.4 Key Findings",
    "text": "6.4 Key Findings\n\n6.4.1 Performance Rankings\n\n# By speed (fastest)\nspeed_rank &lt;- all_timing %&gt;%\n  arrange(Time) %&gt;%\n  head(5)\n\ncat(\"Fastest packages (top 5):\\n\")\n\nFastest packages (top 5):\n\nfor (i in 1:nrow(speed_rank)) {\n  cat(sprintf(\"%d. %s (%s): %.1f seconds\\n\",\n              i, speed_rank$Package[i], speed_rank$Language[i], speed_rank$Time[i]))\n}\n\n1. fixest (R): 0.9 seconds\n2. diff_diff (Python): 5.8 seconds\n3. pyfixest (Python): 10.1 seconds\n4. linearmodels (Python): 10.9 seconds\n5. fixest (R): 25.5 seconds\n\n# By accuracy (lowest bias, excluding TWFE)\naccuracy_rank &lt;- all_att %&gt;%\n  filter(!grepl(\"TWFE\", Method)) %&gt;%\n  arrange(abs(Bias)) %&gt;%\n  head(5)\n\ncat(\"\\nMost accurate packages (top 5, excluding TWFE):\\n\")\n\n\nMost accurate packages (top 5, excluding TWFE):\n\nfor (i in 1:nrow(accuracy_rank)) {\n  cat(sprintf(\"%d. %s (%s): Bias = %.4f\\n\",\n              i, accuracy_rank$Package[i], accuracy_rank$Language[i], accuracy_rank$Bias[i]))\n}\n\n1. did (R): Bias = -0.0023\n2. fixest (R): Bias = -0.0023\n3. diff_diff (Python): Bias = 0.1253\n\n\n\n\n6.4.2 Summary Table\n\nsummary_table &lt;- all_timing %&gt;%\n  left_join(all_att %&gt;% select(Package, Method, Language, ATT, Bias),\n            by = c(\"Package\", \"Method\", \"Language\")) %&gt;%\n  arrange(Time)\n\nkable(summary_table,\n      digits = c(0, 0, 2, 0, 4, 4),\n      caption = \"Complete Results Summary (sorted by execution time)\")\n\n\nComplete Results Summary (sorted by execution time)\n\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nTime\nLanguage\nATT\nBias\n\n\n\n\nfixest\nTraditional TWFE (biased)\n0.94\nR\n1.3286\n-0.2583\n\n\ndiff_diff\nCallaway & Sant’Anna\n5.77\nPython\n1.7122\n0.1253\n\n\npyfixest\nTWFE (sunab not available)\n10.07\nPython\n1.3286\n-0.2583\n\n\nlinearmodels\nTraditional TWFE (biased)\n10.94\nPython\n1.3286\n-0.2583\n\n\nfixest\nSun & Abraham (sunab)\n25.49\nR\n1.5846\n-0.0023\n\n\ndid\nCallaway & Sant’Anna\n30.05\nR\n1.5846\n-0.0023\n\n\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille (10% sample)\n71.35\nR\nNA\nNA",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#recommendations",
    "href": "comparison.html#recommendations",
    "title": "5  Summary and Comparison",
    "section": "6.5 Recommendations",
    "text": "6.5 Recommendations\nBased on our benchmarks with 1 million observations:\n\n6.5.1 For R Users\n\n\n\nUse Case\nRecommended Package\nNotes\n\n\n\n\nSpeed priority\nfixest::sunab\nExtremely fast, good accuracy\n\n\nMost established\ndid\nOriginal C&S implementation\n\n\nImputation approach\ndidimputation\nGood for complex designs\n\n\nRobustness checks\nDIDmultiplegtDYN\nMemory-intensive\n\n\n\n\n\n6.5.2 For Python Users\n\n\n\nUse Case\nRecommended Package\nNotes\n\n\n\n\nSpeed priority\ndiff_diff\nFast C&S implementation\n\n\nStandard C&S\ncsdid\nPort of R package\n\n\nFixed effects\npyfixest\nGood for TWFE comparison\n\n\n\n\n\n6.5.3 Missing in Python\n\n\n\n\n\n\nPackage Only Available in R/Stata\n\n\n\nUsers requiring this method must use R or Stata:\nBorusyak, Jaravel & Spiess Imputation - didimputation (R) / did_imputation (Stata)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#conclusion",
    "href": "comparison.html#conclusion",
    "title": "5  Summary and Comparison",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nAll heterogeneity-robust estimators (Callaway & Sant’Anna, Sun & Abraham, de Chaisemartin & D’Haultfoeuille, Imputation) correctly recover the true treatment effect, while traditional TWFE shows bias due to heterogeneous effects.\nFor large datasets (1M+ observations):\n\nR offers the most comprehensive ecosystem with all methods available\nPython has good support for Callaway & Sant’Anna but lacks implementations of some methods\nSpeed: fixest::sunab (R) and diff_diff (Python) are the fastest\nAccuracy: All robust methods perform similarly well\n\nThe choice of package should depend on:\n\nYour programming language preference\nSpeed requirements for your data size\nWhether you need specific estimators not available in Python",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "",
    "text": "1 Overview\nThis book provides a comprehensive comparison of difference-in-differences (DiD) estimators designed for staggered treatment adoption with heterogeneous treatment effects. We benchmark multiple R and Python packages using synthetic data following the simulation design from @callaway2021difference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.1 Background",
    "text": "1.1 Background\nTraditional two-way fixed effects (TWFE) estimators can produce biased estimates when:\n\nTreatment is adopted at different times across units (staggered rollout)\nTreatment effects vary over time or across cohorts (heterogeneous effects)\n\nSeveral new estimators have been developed to address these issues, including methods by:\n\nCallaway & Sant’Anna (2021): Group-time average treatment effects\nde Chaisemartin & D’Haultfoeuille (2020, 2024): Robust DiD with multiple periods\nSun & Abraham (2021): Interaction-weighted estimator\nBorusyak, Jaravel & Spiess (2024): Imputation-based approach",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#packages-compared",
    "href": "index.html#packages-compared",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.2 Packages Compared",
    "text": "1.2 Packages Compared\n\n1.2.1 R Packages\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nAuthors\nAvailable\n\n\n\n\ndid\nGroup-time ATT\nCallaway & Sant’Anna\nYes\n\n\nDIDmultiplegt / DIDmultiplegtDYN\nRobust DiD\nde Chaisemartin & D’Haultfoeuille\nYes\n\n\nfixest (sunab)\nInteraction-weighted\nSun & Abraham\nYes\n\n\ndidimputation\nImputation\nBorusyak, Jaravel & Spiess\nYes\n\n\n\n\n\n1.2.2 Python Packages\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nAuthors\nAvailable\n\n\n\n\ncsdid\nGroup-time ATT\nCallaway & Sant’Anna (port)\nYes\n\n\ndiff_diff\nMultiple methods\nCommunity\nYes\n\n\ndid_multiplegt_dyn\nRobust DiD\nde Chaisemartin & D’Haultfoeuille\nNo native Python package\n\n\npyfixest\nSun & Abraham via feols\nCommunity port\nYes (partial)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#simulation-design",
    "href": "index.html#simulation-design",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.3 Simulation Design",
    "text": "1.3 Simulation Design\nWe generate synthetic panel data with:\n\n1,000,000 units observed over 10 time periods\nStaggered treatment adoption: Units receive treatment at different times (2012, 2014, 2016, 2018) or never\nHeterogeneous treatment effects: Effects vary by treatment cohort and time since treatment\nUnit and time fixed effects\n\nThis design allows us to test whether each estimator correctly recovers the true treatment effects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#metrics",
    "href": "index.html#metrics",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.4 Metrics",
    "text": "1.4 Metrics\nFor each package, we report:\n\nExecution time: How long the estimation takes\nEstimated ATT: Compared to the true effect\nEvent study estimates: Dynamic effects by time relative to treatment\nMemory usage (where available)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Difference-in-Differences with Staggered Treatment: Package Comparison",
    "section": "1.5 References",
    "text": "1.5 References\n\nCallaway, B., & Sant’Anna, P. H. (2021). Difference-in-differences with multiple time periods. Journal of Econometrics.\nde Chaisemartin, C., & D’Haultfoeuille, X. (2020). Two-way fixed effects estimators with heterogeneous treatment effects. American Economic Review.\nSun, L., & Abraham, S. (2021). Estimating dynamic treatment effects in event studies with heterogeneous treatment effects. Journal of Econometrics.\nBorusyak, K., Jaravel, X., & Spiess, J. (2024). Revisiting event study designs: Robust and efficient estimation. Review of Economic Studies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data_generation.html",
    "href": "data_generation.html",
    "title": "2  Synthetic Data Generation",
    "section": "",
    "text": "3 Data Generation Process\nWe generate synthetic panel data following the simulation design in Callaway & Sant’Anna (2021), with heterogeneous treatment effects that vary by:\nThis design creates a challenging test case where traditional TWFE would produce biased estimates.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#data-generating-process",
    "href": "data_generation.html#data-generating-process",
    "title": "2  Synthetic Data Generation",
    "section": "3.1 Data Generating Process",
    "text": "3.1 Data Generating Process\nThe outcome variable follows:\n\\[Y_{it} = \\alpha_i + \\lambda_t + \\tau_{g,t} \\cdot D_{it} + \\varepsilon_{it}\\]\nWhere:\n\n\\(\\alpha_i\\): Unit fixed effect\n\\(\\lambda_t\\): Time fixed effect\n\\(D_{it}\\): Treatment indicator (1 if unit \\(i\\) is treated at time \\(t\\))\n\\(\\tau_{g,t}\\): Treatment effect that varies by cohort \\(g\\) and calendar time \\(t\\)\n\\(\\varepsilon_{it} \\sim N(0, 1)\\): Idiosyncratic error\n\n\n3.1.1 Heterogeneous Treatment Effects\nWe specify treatment effects that:\n\nIncrease with time since treatment (dynamic effects)\nVary by treatment cohort (cohort heterogeneity)\n\n\\[\\tau_{g,t} = \\tau_0 + \\delta_g + \\gamma \\cdot (t - g)\\]\nWhere:\n\n\\(\\tau_0 = 1.0\\): Base treatment effect\n\\(\\delta_g\\): Cohort-specific shift (earlier cohorts have larger effects)\n\\(\\gamma = 0.1\\): Effect growth per period since treatment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#r-implementation",
    "href": "data_generation.html#r-implementation",
    "title": "2  Synthetic Data Generation",
    "section": "3.2 R Implementation",
    "text": "3.2 R Implementation\n\nset.seed(20240115)\n\n# Parameters\nn_units &lt;- 10000\nn_periods &lt;- 10\nbase_year &lt;- 2010\nyears &lt;- base_year:(base_year + n_periods - 1)\n\n# Treatment cohorts and probabilities\ntreat_cohorts &lt;- c(0, 2012, 2014, 2016, 2018)  # 0 = never treated\ncohort_probs &lt;- c(0.30, 0.20, 0.20, 0.20, 0.10)\n\n# Cohort-specific effect shifts (earlier cohorts have larger effects)\ncohort_effects &lt;- c(0, 0.5, 0.3, 0.1, 0.0)  # delta_g for each cohort\nnames(cohort_effects) &lt;- treat_cohorts\n\n# Base effect and dynamic effect parameter\ntau_0 &lt;- 1.0\ngamma &lt;- 0.1  # Effect growth per period\n\ncat(\"Generating panel data...\\n\")\n\nGenerating panel data...\n\ncat(\"Units:\", format(n_units, big.mark = \",\"), \"\\n\")\n\nUnits: 10,000 \n\ncat(\"Periods:\", n_periods, \"\\n\")\n\nPeriods: 10 \n\ncat(\"Total observations:\", format(n_units * n_periods, big.mark = \",\"), \"\\n\")\n\nTotal observations: 1e+05 \n\n# Generate unit-level data\nunit_data &lt;- data.frame(\n  id = 1:n_units,\n  g = sample(treat_cohorts, n_units, replace = TRUE, prob = cohort_probs),\n  unit_fe = rnorm(n_units, 0, 1)\n)\n\n# Time fixed effects\ntime_fe_vals &lt;- seq(-0.2, 0.2, length.out = n_periods)\nnames(time_fe_vals) &lt;- years\n\n# Expand to panel\ncat(\"Creating panel structure...\\n\")\n\nCreating panel structure...\n\npanel &lt;- expand.grid(id = 1:n_units, year = years)\npanel &lt;- merge(panel, unit_data, by = \"id\")\n\n# Add time fixed effects\npanel$time_fe &lt;- time_fe_vals[as.character(panel$year)]\n\n# Treatment indicator\npanel$treated &lt;- (panel$g &gt; 0) & (panel$year &gt;= panel$g)\n\n# Calculate heterogeneous treatment effects\n# tau_gt = tau_0 + delta_g + gamma * (t - g) for treated observations\npanel$tau_gt &lt;- 0\ntreated_idx &lt;- panel$treated\npanel$tau_gt[treated_idx] &lt;- tau_0 +\n  cohort_effects[as.character(panel$g[treated_idx])] +\n  gamma * (panel$year[treated_idx] - panel$g[treated_idx])\n\n# Generate outcome\npanel$y &lt;- 2 + panel$unit_fe + panel$time_fe +\n           panel$tau_gt * as.integer(panel$treated) +\n           rnorm(nrow(panel), 0, 1)\n\n# Rename for standard DiD notation\npanel$first_treat &lt;- panel$g\n\n# Sort for efficiency\npanel &lt;- panel[order(panel$id, panel$year), ]\n\ncat(\"\\nData generation complete!\\n\")\n\n\nData generation complete!\n\ncat(\"Treatment group distribution:\\n\")\n\nTreatment group distribution:\n\nprint(table(unit_data$g))\n\n\n   0 2012 2014 2016 2018 \n2991 2018 1928 2046 1017 \n\n# Save for use in subsequent chapters\nsaveRDS(panel, \"sim_data.rds\")\nwrite.csv(panel[, c(\"id\", \"year\", \"y\", \"first_treat\", \"treated\")],\n          \"sim_data.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#true-treatment-effects",
    "href": "data_generation.html#true-treatment-effects",
    "title": "2  Synthetic Data Generation",
    "section": "3.3 True Treatment Effects",
    "text": "3.3 True Treatment Effects\nLet’s calculate and display the true treatment effects for validation:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Calculate true ATT by cohort and event time\ntrue_effects &lt;- panel %&gt;%\n  filter(treated) %&gt;%\n  mutate(event_time = year - g) %&gt;%\n  group_by(g, event_time) %&gt;%\n  summarise(\n    true_att = mean(tau_gt),\n    n_obs = n(),\n    .groups = \"drop\"\n  )\n\n# Overall ATT\noverall_att &lt;- mean(panel$tau_gt[panel$treated])\ncat(\"Overall true ATT:\", round(overall_att, 4), \"\\n\\n\")\n\nOverall true ATT: 1.5869 \n\n# ATT by cohort\ncat(\"True ATT by cohort:\\n\")\n\nTrue ATT by cohort:\n\ncohort_att &lt;- true_effects %&gt;%\n  group_by(g) %&gt;%\n  summarise(true_att = mean(true_att), .groups = \"drop\")\nprint(cohort_att)\n\n# A tibble: 4 × 2\n      g true_att\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  2012     1.85\n2  2014     1.55\n3  2016     1.25\n4  2018     1.05\n\n# Dynamic effects (event study)\ncat(\"\\nTrue dynamic effects (averaged across cohorts):\\n\")\n\n\nTrue dynamic effects (averaged across cohorts):\n\ndynamic_effects &lt;- true_effects %&gt;%\n  group_by(event_time) %&gt;%\n  summarise(true_att = weighted.mean(true_att, n_obs), .groups = \"drop\")\nprint(dynamic_effects)\n\n# A tibble: 8 × 2\n  event_time true_att\n       &lt;dbl&gt;    &lt;dbl&gt;\n1          0     1.26\n2          1     1.36\n3          2     1.50\n4          3     1.60\n5          4     1.80\n6          5     1.90\n7          6     2.1 \n8          7     2.2 \n\n\n\nlibrary(ggplot2)\n\nggplot(dynamic_effects, aes(x = event_time, y = true_att)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Event Time (periods since treatment)\",\n    y = \"True ATT\",\n    title = \"True Dynamic Treatment Effects\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = -7:7)\n\n\n\n\nTrue treatment effects by event time",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#python-implementation",
    "href": "data_generation.html#python-implementation",
    "title": "2  Synthetic Data Generation",
    "section": "3.4 Python Implementation",
    "text": "3.4 Python Implementation\nFor Python packages, we’ll load the same data:\n\nimport numpy as np\nimport pandas as pd\n\n# Load the data generated by R for consistency\ndf = pd.read_csv(\"sim_data.csv\")\nprint(f\"Loaded {len(df):,} observations\")\nprint(f\"Units: {df['id'].nunique():,}\")\nprint(f\"Periods: {df['year'].nunique()}\")\nprint(f\"\\nTreatment cohort distribution:\")\nprint(df.groupby('first_treat')['id'].nunique())",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "data_generation.html#data-summary",
    "href": "data_generation.html#data-summary",
    "title": "2  Synthetic Data Generation",
    "section": "3.5 Data Summary",
    "text": "3.5 Data Summary\n\ncat(\"Final dataset dimensions:\\n\")\n\nFinal dataset dimensions:\n\ncat(\"Rows:\", format(nrow(panel), big.mark = \",\"), \"\\n\")\n\nRows: 10,000,000 \n\ncat(\"Columns:\", ncol(panel), \"\\n\")\n\nColumns: 9 \n\ncat(\"\\nColumn names:\", paste(names(panel), collapse = \", \"), \"\\n\")\n\n\nColumn names: id, year, g, unit_fe, time_fe, treated, tau_gt, y, first_treat \n\ncat(\"\\nTreatment status:\\n\")\n\n\nTreatment status:\n\ncat(\"Treated observations:\", format(sum(panel$treated), big.mark = \",\"), \"\\n\")\n\nTreated observations: 3,802,782 \n\ncat(\"Control observations:\", format(sum(!panel$treated), big.mark = \",\"), \"\\n\")\n\nControl observations: 6,197,218 \n\ncat(\"\\nMemory usage:\", format(object.size(panel), units = \"MB\"), \"\\n\")\n\n\nMemory usage: 610.4 Mb",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Synthetic Data Generation</span>"
    ]
  },
  {
    "objectID": "r_analysis.html",
    "href": "r_analysis.html",
    "title": "3  R Package Analysis",
    "section": "",
    "text": "4 R Packages for Staggered DiD\nThis chapter benchmarks four major R packages for difference-in-differences with staggered treatment adoption.\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the simulated data\npanel &lt;- readRDS(\"sim_data.rds\")\n\ncat(\"Data loaded:\\n\")\n\nData loaded:\n\ncat(\"Observations:\", format(nrow(panel), big.mark = \",\"), \"\\n\")\n\nObservations: 10,000,000 \n\ncat(\"Units:\", format(length(unique(panel$id)), big.mark = \",\"), \"\\n\")\n\nUnits: 1,000,000 \n\n# Calculate true overall ATT for comparison\ntrue_overall_att &lt;- mean(panel$tau_gt[panel$treated])\ncat(\"True overall ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue overall ATT: 1.5869 \n\n# Initialize results storage\nresults &lt;- list()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#callaway-santanna-did-package",
    "href": "r_analysis.html#callaway-santanna-did-package",
    "title": "3  R Package Analysis",
    "section": "4.1 1. Callaway & Sant’Anna (did package)",
    "text": "4.1 1. Callaway & Sant’Anna (did package)\nThe did package implements the method from Callaway & Sant’Anna (2021), estimating group-time average treatment effects and aggregating them.\nMethod: Doubly-robust estimation combining outcome regression and propensity score weighting.\n\nlibrary(did)\n\ncat(\"Running did::att_gt()...\\n\")\n\nRunning did::att_gt()...\n\ncat(\"This may take several minutes with 1M units.\\n\\n\")\n\nThis may take several minutes with 1M units.\n\n# Time the estimation\nstart_time &lt;- Sys.time()\n\n# Estimate group-time ATTs\n# Using subset for large data (full data may require significant memory)\nout_cs &lt;- att_gt(\n  yname = \"y\",\n  gname = \"first_treat\",\n  idname = \"id\",\n  tname = \"year\",\n  data = panel,\n  control_group = \"nevertreated\",\n  est_method = \"dr\",  # doubly robust\n  base_period = \"universal\"\n)\n\ncs_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"\\nExecution time:\", round(cs_time, 2), \"seconds\\n\")\n\n\nExecution time: 31.49 seconds\n\n# Aggregate to overall ATT\nagg_cs &lt;- aggte(out_cs, type = \"simple\")\ncat(\"\\nOverall ATT estimate:\", round(agg_cs$overall.att, 4), \"\\n\")\n\n\nOverall ATT estimate: 1.5846 \n\ncat(\"Standard error:\", round(agg_cs$overall.se, 4), \"\\n\")\n\nStandard error: 0.0019 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\n# Event study aggregation\nes_cs &lt;- aggte(out_cs, type = \"dynamic\")\n\nresults$did &lt;- list(\n  package = \"did\",\n  method = \"Callaway & Sant'Anna\",\n  time = cs_time,\n  att = agg_cs$overall.att,\n  se = agg_cs$overall.se,\n  event_study = es_cs\n)\n\n\n# Plot event study\nggdid(es_cs) +\n  labs(title = \"did package: Event Study\",\n       subtitle = paste(\"Execution time:\", round(cs_time, 1), \"seconds\"))\n\n\n\n\nEvent study: Callaway & Sant’Anna (did package)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#de-chaisemartin-dhaultfoeuille-didmultiplegtdyn",
    "href": "r_analysis.html#de-chaisemartin-dhaultfoeuille-didmultiplegtdyn",
    "title": "3  R Package Analysis",
    "section": "4.2 2. de Chaisemartin & D’Haultfoeuille (DIDmultiplegtDYN)",
    "text": "4.2 2. de Chaisemartin & D’Haultfoeuille (DIDmultiplegtDYN)\nThe DIDmultiplegtDYN package implements the method from de Chaisemartin & D’Haultfoeuille, robust to heterogeneous treatment effects.\nMethod: Comparing switchers to non-switchers at each period.\n\n# Check if package is available\nif (requireNamespace(\"DIDmultiplegtDYN\", quietly = TRUE)) {\n  library(DIDmultiplegtDYN)\n  library(dplyr)  # for pipe operator\n\n  cat(\"Running DIDmultiplegtDYN::did_multiplegt_dyn()...\\n\")\n  cat(\"This estimator can be slow with large datasets.\\n\\n\")\n\n  # Prepare data - DIDmultiplegtDYN needs treatment as binary\n  panel_dcdh &lt;- panel %&gt;%\n    mutate(D = as.integer(treated)) %&gt;%\n    select(id, year, y, D)\n\n  start_time &lt;- Sys.time()\n\n  # Note: For very large datasets, this may require sampling\n  # The package is computationally intensive\n  tryCatch({\n    out_dcdh &lt;- did_multiplegt_dyn(\n      df = panel_dcdh,\n      outcome = \"y\",\n      group = \"id\",\n      time = \"year\",\n      treatment = \"D\",\n      effects = 5,  # Number of event-study effects\n      placebo = 3,  # Number of pre-treatment placebos\n      cluster = \"id\",\n      graph_off = TRUE\n    )\n\n    dcdh_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"\\nExecution time:\", round(dcdh_time, 2), \"seconds\\n\")\n    print(summary(out_dcdh))\n\n    # Extract ATT from results$ATE\n    dcdh_att &lt;- out_dcdh$results$ATE\n\n    cat(\"\\nAverage Total Effect (ATT):\", round(dcdh_att, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didmultiplegt &lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille\",\n      time = dcdh_time,\n      att = dcdh_att,\n      output = out_dcdh\n    )\n  }, error = function(e) {\n    cat(\"Error running DIDmultiplegtDYN:\", e$message, \"\\n\")\n    cat(\"This package may have memory constraints with 1M observations.\\n\")\n    cat(\"Consider using a subsample for this estimator.\\n\")\n\n    results$didmultiplegt &lt;&lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille\",\n      time = NA,\n      error = e$message\n    )\n  })\n\n} else {\n  cat(\"Package DIDmultiplegtDYN not installed.\\n\")\n  cat(\"Install with: install.packages('DIDmultiplegtDYN')\\n\")\n\n  results$didmultiplegt &lt;- list(\n    package = \"DIDmultiplegtDYN\",\n    method = \"de Chaisemartin & D'Haultfoeuille\",\n    time = NA,\n    error = \"Package not installed\"\n  )\n}\n\nRunning DIDmultiplegtDYN::did_multiplegt_dyn()...\nThis estimator can be slow with large datasets.\n\nError running DIDmultiplegtDYN: vector memory limit of 36.0 Gb reached, see mem.maxVSize() \nThis package may have memory constraints with 1M observations.\nConsider using a subsample for this estimator.\n\n\n\n4.2.1 Alternative: Using a subsample\nDue to computational constraints, we may need to use a subsample:\n\nif (requireNamespace(\"DIDmultiplegtDYN\", quietly = TRUE) &&\n    (is.null(results$didmultiplegt$time) || is.na(results$didmultiplegt$time))) {\n\n  library(DIDmultiplegtDYN)\n  library(dplyr)  # for pipe operator\n\n  cat(\"Running DIDmultiplegtDYN on 10% subsample...\\n\")\n\n  # Sample 10% of units\n  set.seed(123)\n  sample_ids &lt;- sample(unique(panel$id), size = 100000)\n  panel_sample &lt;- panel %&gt;%\n    filter(id %in% sample_ids) %&gt;%\n    mutate(D = as.integer(treated)) %&gt;%\n    select(id, year, y, D)\n\n  cat(\"Subsample size:\", format(nrow(panel_sample), big.mark = \",\"), \"\\n\\n\")\n\n  start_time &lt;- Sys.time()\n\n  tryCatch({\n    out_dcdh_sample &lt;- did_multiplegt_dyn(\n      df = panel_sample,\n      outcome = \"y\",\n      group = \"id\",\n      time = \"year\",\n      treatment = \"D\",\n      effects = 5,\n      placebo = 3,\n      cluster = \"id\",\n      graph_off = TRUE\n    )\n\n    dcdh_time_sample &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"\\nExecution time (10% sample):\", round(dcdh_time_sample, 2), \"seconds\\n\")\n    cat(\"Estimated full data time:\", round(dcdh_time_sample * 10, 2), \"seconds\\n\")\n    print(summary(out_dcdh_sample))\n\n    # Extract ATT from results$ATE\n    dcdh_att_sample &lt;- out_dcdh_sample$results$ATE\n\n    cat(\"\\nAverage Total Effect (ATT):\", round(dcdh_att_sample, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didmultiplegt_sample &lt;- list(\n      package = \"DIDmultiplegtDYN\",\n      method = \"de Chaisemartin & D'Haultfoeuille (10% sample)\",\n      time = dcdh_time_sample,\n      att = dcdh_att_sample,\n      estimated_full_time = dcdh_time_sample * 10,\n      output = out_dcdh_sample\n    )\n  }, error = function(e) {\n    cat(\"Error:\", e$message, \"\\n\")\n  })\n}\n\nRunning DIDmultiplegtDYN on 10% subsample...\nSubsample size: 1,000,000 \n\n\nExecution time (10% sample): 76.09 seconds\nEstimated full data time: 760.9 seconds\n\n----------------------------------------------------------------------\n       Estimation of treatment effects: Event-study effects\n----------------------------------------------------------------------\n             Estimate SE      LB CI   UB CI   N       Switchers\nEffect_1     1.23956  0.00624 1.22734 1.25179 279,385 70,318   \nEffect_2     1.35506  0.00621 1.34288 1.36723 279,385 70,318   \nEffect_3     1.49808  0.00708 1.48420 1.51196 189,344 60,368   \nEffect_4     1.59630  0.00709 1.58241 1.61020 189,344 60,368   \nEffect_5     1.79333  0.00890 1.77589 1.81077 109,652 40,338   \n\nTest of joint nullity of the effects : p-value = 0.0000\n----------------------------------------------------------------------\n    Average cumulative (total) effect per treatment unit\n----------------------------------------------------------------------\n Estimate        SE     LB CI     UB CI         N Switchers \n  1.46362   0.00528   1.45327   1.47398   719,844   301,710 \nAverage number of time periods over which a treatment effect is accumulated: 2.7683\n\n----------------------------------------------------------------------\n     Testing the parallel trends and no anticipation assumptions\n----------------------------------------------------------------------\n             Estimate SE      LB CI    UB CI    N       Switchers\nPlacebo_1    -0.00871 0.00624 -0.02094 0.00353  279,385 70,318   \nPlacebo_2    -0.01505 0.00749 -0.02974 -0.00037 179,385 50,409   \nPlacebo_3    -0.00406 0.00887 -0.02144 0.01333  109,773 40,459   \n\nTest of joint nullity of the placebos : p-value = 0.2133\n\n\nThe development of this package was funded by the European Union.\nERC REALLYCREDIBLE - GA N. 101043899\nNULL\n\nAverage Total Effect (ATT): 1.4636 0.0053 1.4533 1.474 719844 301710 719844 301710 \nTrue ATT: 1.5869",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#sun-abraham-fixestsunab",
    "href": "r_analysis.html#sun-abraham-fixestsunab",
    "title": "3  R Package Analysis",
    "section": "4.3 3. Sun & Abraham (fixest::sunab)",
    "text": "4.3 3. Sun & Abraham (fixest::sunab)\nThe fixest package provides the interaction-weighted estimator from Sun & Abraham (2021) through the sunab() function.\nMethod: Cohort-specific coefficients with clean controls.\n\nlibrary(fixest)\n\ncat(\"Running fixest::feols() with sunab()...\\n\\n\")\n\nRunning fixest::feols() with sunab()...\n\n# Create cohort variable for sunab\npanel_sa &lt;- panel %&gt;%\n  mutate(\n    cohort = ifelse(first_treat == 0, Inf, first_treat),  # Never-treated = Inf\n    rel_time = ifelse(first_treat == 0, -1000, year - first_treat)  # Relative time\n  )\n\nstart_time &lt;- Sys.time()\n\n# Sun & Abraham estimation using sunab\nout_sa &lt;- feols(\n  y ~ sunab(cohort, year) | id + year,\n  data = panel_sa,\n  cluster = ~id\n)\n\nsa_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"Execution time:\", round(sa_time, 2), \"seconds\\n\\n\")\n\nExecution time: 27.16 seconds\n\n# Summary\nsummary(out_sa, agg = \"ATT\")\n\nOLS estimation, Dep. Var.: y\nObservations: 10,000,000\nFixed-effects: id: 1,000,000,  year: 10\nStandard-errors: Clustered (id) \n    Estimate Std. Error t value  Pr(&gt;|t|)    \nATT  1.58456   0.001775 892.853 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.948956     Adj. R2: 0.636526\n                 Within R2: 0.14887 \n\n# Get aggregated ATT - fixest aggregate returns different structure\nagg_sa &lt;- summary(out_sa, agg = \"ATT\")\n\n# Extract ATT from the aggregated coefficients\nsa_coefs &lt;- coef(agg_sa)\nsa_ses &lt;- se(agg_sa)\n\n# Filter to post-treatment effects (event time &gt;= 0) and compute mean\npost_idx &lt;- grep(\"^year::\", names(sa_coefs))\nif (length(post_idx) &gt; 0) {\n  # Get event times from coefficient names\n  event_times &lt;- as.numeric(gsub(\"year::\", \"\", names(sa_coefs)[post_idx]))\n  post_treatment &lt;- event_times &gt;= 0\n  sa_att &lt;- mean(sa_coefs[post_idx][post_treatment], na.rm = TRUE)\n  sa_se &lt;- mean(sa_ses[post_idx][post_treatment], na.rm = TRUE)\n} else {\n  sa_att &lt;- mean(sa_coefs, na.rm = TRUE)\n  sa_se &lt;- mean(sa_ses, na.rm = TRUE)\n}\n\ncat(\"\\nOverall ATT estimate (post-treatment avg):\", round(sa_att, 4), \"\\n\")\n\n\nOverall ATT estimate (post-treatment avg): 1.5846 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\nresults$sunab &lt;- list(\n  package = \"fixest\",\n  method = \"Sun & Abraham (sunab)\",\n  time = sa_time,\n  att = sa_att,\n  se = sa_se,\n  model = out_sa\n)\n\n\n# Event study plot\niplot(out_sa,\n      main = paste(\"fixest::sunab Event Study\\nExecution time:\",\n                   round(sa_time, 1), \"seconds\"))\n\n\n\n\nEvent study: Sun & Abraham (fixest::sunab)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#imputation-estimator-didimputation",
    "href": "r_analysis.html#imputation-estimator-didimputation",
    "title": "3  R Package Analysis",
    "section": "4.4 4. Imputation Estimator (didimputation)",
    "text": "4.4 4. Imputation Estimator (didimputation)\nThe didimputation package implements the imputation approach from Borusyak, Jaravel & Spiess (2024).\nMethod: Impute counterfactual outcomes for treated units using never-treated/not-yet-treated.\n\nif (requireNamespace(\"didimputation\", quietly = TRUE)) {\n  library(didimputation)\n\n  cat(\"Running didimputation::did_imputation()...\\n\\n\")\n\n  # Prepare data\n  panel_imp &lt;- panel %&gt;%\n    mutate(\n      first_treat = ifelse(first_treat == 0, NA_integer_, first_treat),\n      rel_time = ifelse(is.na(first_treat), NA_integer_, year - first_treat)\n    )\n\n  start_time &lt;- Sys.time()\n\n  tryCatch({\n    out_imp &lt;- did_imputation(\n      data = panel_imp,\n      yname = \"y\",\n      gname = \"first_treat\",\n      tname = \"year\",\n      idname = \"id\",\n      first_stage = ~ 0 | id + year,\n      horizon = TRUE,\n      pretrends = TRUE\n    )\n\n    imp_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\n    cat(\"Execution time:\", round(imp_time, 2), \"seconds\\n\\n\")\n    print(out_imp)\n\n    # Extract overall ATT\n    att_rows &lt;- out_imp$term &gt;= 0\n    overall_att_imp &lt;- mean(out_imp$estimate[att_rows])\n\n    cat(\"\\nOverall ATT (post-treatment avg):\", round(overall_att_imp, 4), \"\\n\")\n    cat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\n    results$didimputation &lt;- list(\n      package = \"didimputation\",\n      method = \"Borusyak, Jaravel & Spiess\",\n      time = imp_time,\n      att = overall_att_imp,\n      output = out_imp\n    )\n  }, error = function(e) {\n    cat(\"Error running didimputation:\", e$message, \"\\n\")\n    results$didimputation &lt;&lt;- list(\n      package = \"didimputation\",\n      method = \"Borusyak, Jaravel & Spiess\",\n      time = NA,\n      error = e$message\n    )\n  })\n\n} else {\n  cat(\"Package didimputation not installed.\\n\")\n  cat(\"Install with: install.packages('didimputation')\\n\")\n\n  results$didimputation &lt;- list(\n    package = \"didimputation\",\n    method = \"Borusyak, Jaravel & Spiess\",\n    time = NA,\n    error = \"Package not installed\"\n  )\n}\n\nRunning didimputation::did_imputation()...\n\n\nError running didimputation: LU factorization of .gCMatrix failed: out of memory or near-singular \n\n\n\nif (!is.null(results$didimputation$output)) {\n  out_imp &lt;- results$didimputation$output\n  imp_time &lt;- results$didimputation$time\n\n  ggplot(out_imp, aes(x = term, y = estimate)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_vline(xintercept = -0.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      x = \"Event Time\",\n      y = \"Estimate\",\n      title = \"didimputation Event Study\",\n      subtitle = paste(\"Execution time:\", round(imp_time, 1), \"seconds\")\n    ) +\n    theme_minimal()\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#traditional-twfe-biased-baseline",
    "href": "r_analysis.html#traditional-twfe-biased-baseline",
    "title": "3  R Package Analysis",
    "section": "4.5 5. Traditional TWFE (Biased Baseline)",
    "text": "4.5 5. Traditional TWFE (Biased Baseline)\nFor comparison, let’s also run traditional two-way fixed effects, which is known to be biased with heterogeneous effects:\n\nlibrary(fixest)\n\ncat(\"Running traditional TWFE for comparison...\\n\\n\")\n\nRunning traditional TWFE for comparison...\n\nstart_time &lt;- Sys.time()\n\nout_twfe &lt;- feols(\n  y ~ treated | id + year,\n  data = panel,\n  cluster = ~id\n)\n\ntwfe_time &lt;- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n\ncat(\"Execution time:\", round(twfe_time, 2), \"seconds\\n\\n\")\n\nExecution time: 1.1 seconds\n\nsummary(out_twfe)\n\nOLS estimation, Dep. Var.: y\nObservations: 10,000,000\nFixed-effects: id: 1,000,000,  year: 10\nStandard-errors: Clustered (id) \n            Estimate Std. Error t value  Pr(&gt;|t|)    \ntreatedTRUE  1.32861   0.001155 1150.25 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.958966     Adj. R2: 0.628819\n                 Within R2: 0.130818\n\ncat(\"\\nTWFE ATT estimate:\", round(coef(out_twfe), 4), \"\\n\")\n\n\nTWFE ATT estimate: 1.3286 \n\ncat(\"True ATT:\", round(true_overall_att, 4), \"\\n\")\n\nTrue ATT: 1.5869 \n\ncat(\"Bias:\", round(coef(out_twfe) - true_overall_att, 4), \"\\n\")\n\nBias: -0.2583 \n\nresults$twfe &lt;- list(\n  package = \"fixest\",\n  method = \"Traditional TWFE (biased)\",\n  time = twfe_time,\n  att = coef(out_twfe),\n  se = se(out_twfe)\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "r_analysis.html#r-results-summary",
    "href": "r_analysis.html#r-results-summary",
    "title": "3  R Package Analysis",
    "section": "4.6 R Results Summary",
    "text": "4.6 R Results Summary\n\n# Create summary table\nsummary_df &lt;- data.frame(\n  Package = character(),\n  Method = character(),\n  Time_seconds = numeric(),\n  ATT_estimate = numeric(),\n  True_ATT = numeric(),\n  Bias = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (name in names(results)) {\n  r &lt;- results[[name]]\n  att &lt;- if (!is.null(r$att)) r$att else NA\n  summary_df &lt;- rbind(summary_df, data.frame(\n    Package = r$package,\n    Method = r$method,\n    Time_seconds = ifelse(is.null(r$time), NA, r$time),\n    ATT_estimate = att,\n    True_ATT = true_overall_att,\n    Bias = ifelse(is.na(att), NA, att - true_overall_att),\n    stringsAsFactors = FALSE\n  ))\n}\n\nknitr::kable(summary_df, digits = 4,\n             caption = \"R Package Comparison Summary\")\n\n\nR Package Comparison Summary\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nTime_seconds\nATT_estimate\nTrue_ATT\nBias\n\n\n\n\n1\ndid\nCallaway & Sant’Anna\n30.0508\n1.5846\n1.5869\n-0.0023\n\n\n2\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille\nNA\nNA\n1.5869\nNA\n\n\n3\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille (10% sample)\n71.3493\nNA\n1.5869\nNA\n\n\n4\nfixest\nSun & Abraham (sunab)\n25.4929\n1.5846\n1.5869\n-0.0023\n\n\n5\ndidimputation\nBorusyak, Jaravel & Spiess\nNA\nNA\n1.5869\nNA\n\n\ntreatedTRUE\nfixest\nTraditional TWFE (biased)\n0.9361\n1.3286\n1.5869\n-0.2583\n\n\n\n\n# Save results for comparison chapter\nsaveRDS(results, \"r_results.rds\")\n\n\n# Filter to valid times\ntiming_df &lt;- summary_df %&gt;%\n  filter(!is.na(Time_seconds))\n\nggplot(timing_df, aes(x = reorder(Package, Time_seconds), y = Time_seconds)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    x = \"Package\",\n    y = \"Execution Time (seconds)\",\n    title = \"R Package Execution Times\",\n    subtitle = paste(\"Dataset:\", format(nrow(panel), big.mark = \",\"), \"observations\")\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = round(Time_seconds, 1)), hjust = -0.1)\n\n\n\n\nExecution time comparison (R packages)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html",
    "href": "python_analysis.html",
    "title": "4  Python Package Analysis",
    "section": "",
    "text": "5 Python Packages for Staggered DiD\nThis chapter benchmarks Python packages for difference-in-differences with staggered treatment. Python has fewer native implementations compared to R, but several community packages exist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#package-availability-summary",
    "href": "python_analysis.html#package-availability-summary",
    "title": "4  Python Package Analysis",
    "section": "5.1 Package Availability Summary",
    "text": "5.1 Package Availability Summary\n\n\n\n\n\n\n\n\n\nMethod\nR Package\nPython Package\nStatus\n\n\n\n\nCallaway & Sant’Anna\ndid\ncsdid, diff_diff\nAvailable\n\n\nde Chaisemartin & D’Haultfoeuille\nDIDmultiplegtDYN\ndid_multiplegt_dyn\nAvailable\n\n\nSun & Abraham\nfixest::sunab\npyfixest\nAvailable (partial)\n\n\nBorusyak, Jaravel & Spiess\ndidimputation\nNone\nNot available in Python\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the data\ndf = pd.read_csv(\"sim_data.csv\")\n\nprint(f\"Data loaded:\")\nprint(f\"Observations: {len(df):,}\")\nprint(f\"Units: {df['id'].nunique():,}\")\nprint(f\"Periods: {df['year'].nunique()}\")\n\n# Calculate true ATT (from treated observations' true effects)\n# We need to regenerate true effects since CSV doesn't have them\ntreat_cohorts = [0, 2012, 2014, 2016, 2018]\ncohort_effects = {0: 0, 2012: 0.5, 2014: 0.3, 2016: 0.1, 2018: 0.0}\ntau_0 = 1.0\ngamma = 0.1\n\ndf['tau_gt'] = 0.0\nmask = df['treated'] == True\ndf.loc[mask, 'tau_gt'] = (\n    tau_0 +\n    df.loc[mask, 'first_treat'].map(cohort_effects) +\n    gamma * (df.loc[mask, 'year'] - df.loc[mask, 'first_treat'])\n)\n\ntrue_overall_att = df.loc[df['treated'], 'tau_gt'].mean()\nprint(f\"\\nTrue overall ATT: {true_overall_att:.4f}\")\n\n# Store results\nresults = {}\n\nData loaded:\nObservations: 100,000\nUnits: 10,000\nPeriods: 10\n\nTrue overall ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#callaway-santanna-csdid",
    "href": "python_analysis.html#callaway-santanna-csdid",
    "title": "4  Python Package Analysis",
    "section": "5.2 1. Callaway & Sant’Anna (csdid)",
    "text": "5.2 1. Callaway & Sant’Anna (csdid)\nThe csdid package is a Python port of the original R did package.\n\ntry:\n    from csdid.att_gt import ATTgt\n\n    print(\"Running csdid ATTgt()...\")\n    print(\"This may take several minutes with 1M units.\\n\")\n\n    start_time = time.perf_counter()\n\n    att_gt = ATTgt(\n        yname='y',\n        gname='first_treat',\n        idname='id',\n        tname='year',\n        data=df\n    )\n\n    out_csdid = att_gt.fit(est_method='dr')  # doubly robust\n\n    csdid_time = time.perf_counter() - start_time\n\n    print(f\"\\nExecution time: {csdid_time:.2f} seconds\")\n\n    # Aggregate to dynamic effects\n    agg_dynamic = out_csdid.aggte(typec='dynamic', na_rm=True)\n\n    # Extract overall ATT from the aggregation\n    csdid_att = agg_dynamic.summ_attgt().atte['overall_att']\n\n    print(f\"\\nEstimated ATT: {csdid_att:.4f}\" if csdid_att else \"ATT not extracted\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': csdid_time,\n        'att': csdid_att,\n        'output': out_csdid\n    }\n\nexcept ImportError as e:\n    print(\"Package csdid not installed.\")\n    print(\"Install with: pip install csdid\")\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running csdid: {e}\")\n    results['csdid'] = {\n        'package': 'csdid',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning csdid ATTgt()...\nThis may take several minutes with 1M units.\n\n\nExecution time: 0.61 seconds\n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:\n   ATT Std. Error [95.0%  Conf. Int.]  \n1.7041     0.0201 1.6647       1.7435 *\n\n\nDynamic Effects:\n    Event time  Estimate  Std. Error  [95.0% Simult.   Conf. Band   \n0           -7    0.0760      0.0493          -0.0207      0.1727   \n1           -6    0.0036      0.0497          -0.0939      0.1010   \n2           -5    0.0034      0.0327          -0.0607      0.0674   \n3           -4   -0.0378      0.0318          -0.1002      0.0245   \n4           -3    0.0141      0.0250          -0.0350      0.0631   \n5           -2   -0.0178      0.0240          -0.0648      0.0292   \n6           -1    0.0108      0.0218          -0.0318      0.0534   \n7            0    1.2330      0.0225           1.1889      1.2771  *\n8            1    1.3592      0.0175           1.3250      1.3934  *\n9            2    1.5132      0.0258           1.4627      1.5637  *\n10           3    1.5721      0.0212           1.5305      1.6137  *\n11           4    1.7740      0.0300           1.7152      1.8329  *\n12           5    1.8917      0.0295           1.8339      1.9496  *\n13           6    2.1220      0.0402           2.0432      2.2008  *\n14           7    2.1674      0.0397           2.0896      2.2453  *\n---\nSignif. codes: `*' confidence band does not cover 0\nControl Group:  Never Treated , \nAnticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\n\nEstimated ATT: 1.7041\nTrue ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#alternative-cs-implementation-diff_diff",
    "href": "python_analysis.html#alternative-cs-implementation-diff_diff",
    "title": "4  Python Package Analysis",
    "section": "5.3 2. Alternative CS Implementation (diff_diff)",
    "text": "5.3 2. Alternative CS Implementation (diff_diff)\nThe diff_diff package provides another implementation of Callaway & Sant’Anna.\n\ntry:\n    from diff_diff import CallawaySantAnna\n\n    print(\"Running diff_diff CallawaySantAnna()...\")\n    print()\n\n    cs = CallawaySantAnna()\n\n    start_time = time.perf_counter()\n\n    cs_results = cs.fit(\n        df,\n        outcome='y',\n        unit='id',\n        time='year',\n        first_treat='first_treat',\n        aggregate='event_study'\n    )\n\n    diff_diff_time = time.perf_counter() - start_time\n\n    print(f\"Execution time: {diff_diff_time:.2f} seconds\")\n\n    # Extract ATT from event study\n    if hasattr(cs_results, 'event_study_effects'):\n        post_effects = [v['effect'] for k, v in cs_results.event_study_effects.items()\n                       if k &gt;= 0]\n        diff_diff_att = np.mean(post_effects) if post_effects else None\n    else:\n        diff_diff_att = None\n\n    print(f\"\\nEstimated ATT (post-treatment avg): {diff_diff_att:.4f}\" if diff_diff_att else \"\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': diff_diff_time,\n        'att': diff_diff_att,\n        'output': cs_results\n    }\n\nexcept ImportError as e:\n    print(\"Package diff_diff not installed.\")\n    print(\"Install with: pip install diff-diff\")\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running diff_diff: {e}\")\n    results['diff_diff'] = {\n        'package': 'diff_diff',\n        'method': 'Callaway & Sant\\'Anna',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning diff_diff CallawaySantAnna()...\n\nExecution time: 0.05 seconds\n\nEstimated ATT (post-treatment avg): 1.7041\nTrue ATT: 1.5861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#sun-abraham-via-pyfixest",
    "href": "python_analysis.html#sun-abraham-via-pyfixest",
    "title": "4  Python Package Analysis",
    "section": "5.4 3. Sun & Abraham via pyfixest",
    "text": "5.4 3. Sun & Abraham via pyfixest\nThe pyfixest package is a Python port of the R fixest package and includes support for Sun & Abraham estimation.\n\ntry:\n    import pyfixest as pf\n\n    print(\"Running pyfixest for Sun & Abraham...\")\n    print()\n\n    # Prepare data for pyfixest\n    df_pf = df.copy()\n    df_pf['cohort'] = df_pf['first_treat'].replace(0, np.inf)  # Never-treated = Inf\n\n    start_time = time.perf_counter()\n\n    # Sun & Abraham estimation\n    # pyfixest uses i() for interaction-weighted estimator\n    try:\n        # Try sunab-style estimation\n        out_pf = pf.feols(\n            \"y ~ sunab(cohort, year) | id + year\",\n            data=df_pf,\n            vcov={'CRV1': 'id'}\n        )\n        pf_time = time.perf_counter() - start_time\n\n        print(f\"Execution time: {pf_time:.2f} seconds\")\n        print(out_pf.summary())\n\n        # Extract ATT\n        pf_att = out_pf.coef().mean()  # Average of event study coefficients\n\n        results['pyfixest'] = {\n            'package': 'pyfixest',\n            'method': 'Sun & Abraham',\n            'time': pf_time,\n            'att': pf_att,\n            'output': out_pf\n        }\n\n    except Exception as e:\n        # Fallback to simple TWFE\n        print(f\"sunab not available in pyfixest, running TWFE: {e}\")\n\n        out_pf = pf.feols(\n            \"y ~ treated | id + year\",\n            data=df_pf,\n            vcov={'CRV1': 'id'}\n        )\n        pf_time = time.perf_counter() - start_time\n\n        print(f\"\\nExecution time (TWFE): {pf_time:.2f} seconds\")\n        print(out_pf.summary())\n\n        results['pyfixest'] = {\n            'package': 'pyfixest',\n            'method': 'TWFE (sunab not available)',\n            'time': pf_time,\n            'att': out_pf.coef()['treated'],\n            'output': out_pf\n        }\n\nexcept ImportError as e:\n    print(\"Package pyfixest not installed.\")\n    print(\"Install with: pip install pyfixest\")\n    results['pyfixest'] = {\n        'package': 'pyfixest',\n        'method': 'Sun & Abraham',\n        'time': None,\n        'error': str(e)\n    }\nexcept Exception as e:\n    print(f\"Error running pyfixest: {e}\")\n    results['pyfixest'] = {\n        'package': 'pyfixest',\n        'method': 'Sun & Abraham',\n        'time': None,\n        'error': str(e)\n    }\n\nRunning pyfixest for Sun & Abraham...\n\nsunab not available in pyfixest, running TWFE: Unable to evaluate factor `sunab(cohort, year)`. [NameError: name 'sunab' is not defined]\n\n\n\nExecution time (TWFE): 5.17 seconds\n###\n\nEstimation:  OLS\nDep. var.: y, Fixed effects: id+year\nInference:  CRV1\nObservations:  100000\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| treated       |      1.324 |        0.012 |   114.788 |      0.000 |  1.302 |   1.347 |\n---\nRMSE: 0.958 R2: 0.668 R2 Within: 0.13 \nNone",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#de-chaisemartin-dhaultfoeuille-did_multiplegt_dyn",
    "href": "python_analysis.html#de-chaisemartin-dhaultfoeuille-did_multiplegt_dyn",
    "title": "4  Python Package Analysis",
    "section": "5.5 4. de Chaisemartin & D’Haultfoeuille (did_multiplegt_dyn)",
    "text": "5.5 4. de Chaisemartin & D’Haultfoeuille (did_multiplegt_dyn)\nThe did_multiplegt_dyn package is a Python port of the original Stata/R command by de Chaisemartin & D’Haultfoeuille.\nMethod: Compares switchers to non-switchers at each period, robust to heterogeneous treatment effects.\n\nimport polars as pl\nfrom did_multiplegt_dyn import DidMultiplegtDyn\n\nprint(\"Running DidMultiplegtDyn()...\")\nprint(\"Note: This estimator can be computationally intensive.\\n\")\n\n# Convert pandas DataFrame to polars and prepare data\ndf_dcdh = pl.from_pandas(df)\ndf_dcdh = df_dcdh.with_columns([\n    pl.col('treated').cast(pl.Int32).alias('D')\n])\nstart_time = time.perf_counter()\n\ntry:\n    # Create model instance\n    model_dcdh = DidMultiplegtDyn(\n        df=df_dcdh,\n        outcome='y',\n        group='id',\n        time='year',\n        treatment='D',\n        effects=5,\n        placebo=3,\n        cluster='id'\n    )\n\n    # Fit the model\n    model_dcdh.fit()\n\n    dcdh_time = time.perf_counter() - start_time\n\n    # Get summary table\n    dcdh_summary = model_dcdh.summary()\n\n    print(f\"\\nExecution time (10% sample): {dcdh_time:.2f} seconds\")\n    print(f\"Estimated full data time: ~{dcdh_time * 10:.0f} seconds\")\n    print()\n    print(dcdh_summary)\n\n    # Extract Average_Total_Effect from summary table\n    dcdh_att = model_dcdh.result['did_multiplegt_dyn']['ATE']['Estimate'].values[0]\n\n    results['did_multiplegt_dyn'] = {\n        'package': 'did_multiplegt_dyn',\n        'method': 'de Chaisemartin & D\\'Haultfoeuille (10% sample)',\n        'time': dcdh_time,\n        'estimated_full_time': dcdh_time * 10,\n        'att': dcdh_att,\n        'output': dcdh_summary\n    }\n\nexcept Exception as e:\n    print(f\"Error during estimation: {e}\")\n    import traceback\n    traceback.print_exc()\n    results['did_multiplegt_dyn'] = {\n        'package': 'did_multiplegt_dyn',\n        'method': 'de Chaisemartin & D\\'Haultfoeuille',\n        'time': None,\n        'att': None,\n        'error': str(e)\n    }\n\nRunning DidMultiplegtDyn()...\nNote: This estimator can be computationally intensive.\n\n               Block  Estimate       SE     LB CI    UB CI       N  Switchers     N.w  Switchers.w\n            Effect_1  1.242962 0.019474  1.204793 1.281131 28044.0     7009.0 28044.0       7009.0\n            Effect_2  1.361619 0.019661  1.323084 1.400153 28044.0     7009.0 28044.0       7009.0\n            Effect_3  1.524482 0.022447  1.480487 1.568477 19045.0     5992.0 19045.0       5992.0\n            Effect_4  1.574469 0.022517  1.530335 1.618602 19045.0     5992.0 19045.0       5992.0\n            Effect_5  1.781573 0.028124  1.726450 1.836696 10945.0     3946.0 10945.0       3946.0\nAverage_Total_Effect  1.464355 0.016578  1.431863 1.496847 72018.0    29948.0 72018.0      29948.0\n           Placebo_1 -0.003321 0.019518 -0.041577 0.034934 28044.0     7009.0 28044.0       7009.0\n           Placebo_2 -0.011834 0.023465 -0.057825 0.034156 18044.0     4991.0 18044.0       4991.0\n           Placebo_3 -0.018693 0.028306 -0.074171 0.036786 10973.0     3974.0 10973.0       3974.0\n\nExecution time (10% sample): 0.65 seconds\nEstimated full data time: ~6 seconds\n\n                  Block  Estimate        SE     LB CI     UB CI        N  \\\n0              Effect_1  1.242962  0.019474  1.204793  1.281131  28044.0   \n1              Effect_2  1.361619  0.019661  1.323084  1.400153  28044.0   \n2              Effect_3  1.524482  0.022447  1.480487  1.568477  19045.0   \n3              Effect_4  1.574469  0.022517  1.530335  1.618602  19045.0   \n4              Effect_5  1.781573  0.028124  1.726450  1.836696  10945.0   \n5  Average_Total_Effect  1.464355  0.016578  1.431863  1.496847  72018.0   \n6             Placebo_1 -0.003321  0.019518 -0.041577  0.034934  28044.0   \n7             Placebo_2 -0.011834  0.023465 -0.057825  0.034156  18044.0   \n8             Placebo_3 -0.018693  0.028306 -0.074171  0.036786  10973.0   \n\n   Switchers      N.w  Switchers.w  \n0     7009.0  28044.0       7009.0  \n1     7009.0  28044.0       7009.0  \n2     5992.0  19045.0       5992.0  \n3     5992.0  19045.0       5992.0  \n4     3946.0  10945.0       3946.0  \n5    29948.0  72018.0      29948.0  \n6     7009.0  28044.0       7009.0  \n7     4991.0  18044.0       4991.0  \n8     3974.0  10973.0       3974.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#borusyak-jaravel-spiess-imputation",
    "href": "python_analysis.html#borusyak-jaravel-spiess-imputation",
    "title": "4  Python Package Analysis",
    "section": "5.6 5. Borusyak, Jaravel & Spiess (Imputation)",
    "text": "5.6 5. Borusyak, Jaravel & Spiess (Imputation)\n\n\n\n\n\n\nNot Available in Python\n\n\n\nThere is no native Python package implementing the imputation estimator from Borusyak, Jaravel & Spiess. This method is only available in:\n\nR: didimputation package\nStata: did_imputation command\n\n\n\n\nprint(\"=\" * 60)\nprint(\"Borusyak, Jaravel & Spiess (did_imputation)\")\nprint(\"=\" * 60)\nprint()\nprint(\"STATUS: NOT AVAILABLE IN PYTHON\")\nprint()\nprint(\"This estimator is only available in:\")\nprint(\"  - R: didimputation package\")\nprint(\"  - Stata: did_imputation command\")\nprint()\n\nresults['didimputation'] = {\n    'package': 'N/A',\n    'method': 'Borusyak, Jaravel & Spiess',\n    'time': None,\n    'att': None,\n    'error': 'No Python implementation available'\n}\n\n============================================================\nBorusyak, Jaravel & Spiess (did_imputation)\n============================================================\n\nSTATUS: NOT AVAILABLE IN PYTHON\n\nThis estimator is only available in:\n  - R: didimputation package\n  - Stata: did_imputation command",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#traditional-twfe-biased-baseline",
    "href": "python_analysis.html#traditional-twfe-biased-baseline",
    "title": "4  Python Package Analysis",
    "section": "5.7 6. Traditional TWFE (Biased Baseline)",
    "text": "5.7 6. Traditional TWFE (Biased Baseline)\nFor comparison, let’s run traditional TWFE using linearmodels or statsmodels:\n\ntry:\n    from linearmodels.panel import PanelOLS\n\n    print(\"Running traditional TWFE with linearmodels...\")\n    print()\n\n    # Prepare panel data structure\n    df_panel = df.set_index(['id', 'year'])\n\n    start_time = time.perf_counter()\n\n    model = PanelOLS(\n        df_panel['y'],\n        df_panel[['treated']].astype(float),\n        entity_effects=True,\n        time_effects=True\n    )\n    out_twfe = model.fit(cov_type='clustered', cluster_entity=True)\n\n    twfe_time = time.perf_counter() - start_time\n\n    print(f\"Execution time: {twfe_time:.2f} seconds\")\n    print(out_twfe.summary.tables[1])\n\n    twfe_att = out_twfe.params['treated']\n    print(f\"\\nTWFE ATT estimate: {twfe_att:.4f}\")\n    print(f\"True ATT: {true_overall_att:.4f}\")\n    print(f\"Bias: {twfe_att - true_overall_att:.4f}\")\n\n    results['twfe_linearmodels'] = {\n        'package': 'linearmodels',\n        'method': 'Traditional TWFE (biased)',\n        'time': twfe_time,\n        'att': twfe_att,\n        'output': out_twfe\n    }\n\nexcept ImportError:\n    print(\"Package linearmodels not installed.\")\n    print(\"Install with: pip install linearmodels\")\n\n    # Fallback to statsmodels\n    try:\n        import statsmodels.api as sm\n        from statsmodels.regression.linear_model import OLS\n\n        print(\"\\nRunning TWFE with statsmodels (demeaned)...\")\n\n        # Demean for fixed effects\n        df_fe = df.copy()\n        df_fe['y_demeaned'] = df_fe.groupby('id')['y'].transform(lambda x: x - x.mean())\n        df_fe['y_demeaned'] = df_fe.groupby('year')['y_demeaned'].transform(lambda x: x - x.mean())\n        df_fe['treated_demeaned'] = df_fe.groupby('id')['treated'].transform(lambda x: x - x.mean())\n        df_fe['treated_demeaned'] = df_fe.groupby('year')['treated_demeaned'].transform(lambda x: x - x.mean())\n\n        start_time = time.perf_counter()\n        model = OLS(df_fe['y_demeaned'], df_fe['treated_demeaned']).fit()\n        twfe_time = time.perf_counter() - start_time\n\n        print(f\"Execution time: {twfe_time:.2f} seconds\")\n        print(f\"TWFE ATT: {model.params[0]:.4f}\")\n\n        results['twfe_statsmodels'] = {\n            'package': 'statsmodels',\n            'method': 'Traditional TWFE (biased)',\n            'time': twfe_time,\n            'att': model.params[0]\n        }\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nexcept Exception as e:\n    print(f\"Error running TWFE: {e}\")\n\nRunning traditional TWFE with linearmodels...\n\nExecution time: 0.09 seconds\n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\ntreated        1.3242     0.0122     108.90     0.0000      1.3003      1.3480\n==============================================================================\n\nTWFE ATT estimate: 1.3242\nTrue ATT: 1.5861\nBias: -0.2620",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#python-results-summary",
    "href": "python_analysis.html#python-results-summary",
    "title": "4  Python Package Analysis",
    "section": "5.8 Python Results Summary",
    "text": "5.8 Python Results Summary\n\nimport pandas as pd\n\n# Create summary table\nsummary_data = []\nfor name, r in results.items():\n    summary_data.append({\n        'Package': r.get('package', 'N/A'),\n        'Method': r.get('method', 'N/A'),\n        'Time (s)': r.get('time'),\n        'ATT': r.get('att'),\n        'True ATT': true_overall_att if r.get('time') else None,\n        'Bias': (r.get('att') - true_overall_att) if r.get('att') else None,\n        'Status': 'Error: ' + r.get('error', '') if r.get('error') else 'OK'\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PYTHON PACKAGE COMPARISON SUMMARY\")\nprint(\"=\" * 80)\nprint(summary_df.to_string(index=False))\n\n# Save for comparison chapter\nsummary_df.to_csv(\"python_results.csv\", index=False)\n\n\n================================================================================\nPYTHON PACKAGE COMPARISON SUMMARY\n================================================================================\n           Package                                         Method  Time (s)      ATT  True ATT      Bias                                    Status\n             csdid                           Callaway & Sant'Anna  0.607066 1.704085  1.586146  0.117939                                        OK\n         diff_diff                           Callaway & Sant'Anna  0.049023 1.704085  1.586146  0.117939                                        OK\n          pyfixest                     TWFE (sunab not available)  5.165600 1.324159  1.586146 -0.261987                                        OK\ndid_multiplegt_dyn de Chaisemartin & D'Haultfoeuille (10% sample)  0.645394 1.464355  1.586146 -0.121791                                        OK\n               N/A                     Borusyak, Jaravel & Spiess       NaN      NaN       NaN       NaN Error: No Python implementation available\n      linearmodels                      Traditional TWFE (biased)  0.087213 1.324159  1.586146 -0.261987                                        OK\n\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to packages with valid times\ntiming_data = [(r['package'], r['time']) for r in results.values()\n               if r.get('time') is not None]\n\nif timing_data:\n    packages, times = zip(*timing_data)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.barh(packages, times, color='steelblue')\n    ax.set_xlabel('Execution Time (seconds)')\n    ax.set_title(f'Python Package Execution Times\\nDataset: {len(df):,} observations')\n\n    # Add time labels\n    for bar, t in zip(bars, times):\n        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n                f'{t:.1f}s', va='center')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No timing data available to plot.\")\n\n\n\n\nExecution time comparison (Python packages)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "python_analysis.html#python-package-availability-notes",
    "href": "python_analysis.html#python-package-availability-notes",
    "title": "4  Python Package Analysis",
    "section": "5.9 Python Package Availability Notes",
    "text": "5.9 Python Package Availability Notes\n\n5.9.1 Available Packages\n\ncsdid: Full implementation of Callaway & Sant’Anna\n\nInstall: pip install csdid\nSupports doubly-robust estimation\nCan be slow with very large datasets\n\ndiff_diff: Alternative CS implementation\n\nInstall: pip install diff-diff\nGenerally faster than csdid\nGood for event study aggregation\n\npyfixest: Port of R’s fixest\n\nInstall: pip install pyfixest\nFast fixed effects estimation\nSun & Abraham support varies by version\n\ndid_multiplegt_dyn: de Chaisemartin & D’Haultfoeuille\n\nInstall: pip install did-multiplegt-dyn\nPython port of the R/Stata command\nComputationally intensive for large datasets\n\n\n\n\n5.9.2 Not Available in Python\n\n\n\n\n\n\nMissing Python Implementation\n\n\n\nThe following estimator does not have a native Python package:\nBorusyak, Jaravel & Spiess (didimputation) - Only available in R and Stata - No community Python port exists\nFor this method, users must either:\n\nUse R (recommended via Quarto or rpy2)\nUse Stata\nImplement the method from scratch",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Package Analysis</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html",
    "href": "did_multiplegt_dyn_comparison.html",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "",
    "text": "6 de Chaisemartin & D’Haultfoeuille Estimator Comparison\nThis chapter provides a comprehensive comparison of the did_multiplegt_dyn estimator across Stata, R (CRAN and Polars versions), and Python implementations, with a focus on runtime performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#overview",
    "href": "did_multiplegt_dyn_comparison.html#overview",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nThe did_multiplegt_dyn command estimates event-study Difference-in-Differences (DID) estimators in designs with: - Multiple groups and periods - Potentially non-binary treatment that may increase or decrease multiple times - Heterogeneous treatment effects across groups and time\n\n6.1.1 Available Implementations\n\n\n\n\n\n\n\n\n\nPlatform\nPackage\nSource\nOptimization\n\n\n\n\nStata\ndid_multiplegt_dyn\nSSC\nReference implementation\n\n\nR (CRAN)\nDIDmultiplegtDYN\nCRAN\nStandard R\n\n\nR (Polars)\nDIDmultiplegtDYNpolars\nGitHub/Local\nPolars-optimized\n\n\nPython\npy-did-multiplegt-dyn\nPyPI\nPandas/NumPy",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#runtime-comparison",
    "href": "did_multiplegt_dyn_comparison.html#runtime-comparison",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.2 Runtime Comparison",
    "text": "6.2 Runtime Comparison\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(scales)\n\n# Set paths\nsave_path &lt;- \"CX\"\n\n# Function to safely load CSV\nload_runtime &lt;- function(file, platform) {\n  filepath &lt;- file.path(save_path, file)\n  if (file.exists(filepath)) {\n    df &lt;- read.csv(filepath)\n    df$Platform &lt;- platform\n    return(df)\n  }\n  return(NULL)\n}\n\n# Load all runtime results\nruntime_stata &lt;- load_runtime(\"runtime_stata.csv\", \"Stata\")\nruntime_r_cran &lt;- load_runtime(\"runtime_R_cran.csv\", \"R_CRAN\")\nruntime_r_polars &lt;- load_runtime(\"runtime_R_polars.csv\", \"R_Polars\")\nruntime_python &lt;- load_runtime(\"runtime_python.csv\", \"Python\")\n\n# Combine all results\nall_runtimes &lt;- bind_rows(\n  runtime_stata,\n  runtime_r_cran,\n  runtime_r_polars,\n  runtime_python\n)\n\nif (nrow(all_runtimes) &gt; 0) {\n  cat(\"Runtime data loaded from\", sum(!sapply(list(runtime_stata, runtime_r_cran, runtime_r_polars, runtime_python), is.null)), \"platforms\\n\")\n  cat(\"Total observations:\", nrow(all_runtimes), \"\\n\")\n} else {\n  cat(\"No runtime data found. Please run the test scripts first:\\n\")\n  cat(\"  - Stata: CX/arXiv_replication.do\\n\")\n  cat(\"  - R CRAN: CX/test_did_multiplegt_dyn_cran.R\\n\")\n  cat(\"  - R Polars: CX/test_did_multiplegt_dyn_polars.R\\n\")\n  cat(\"  - Python: CX/test_did_multiplegt_dyn_python.py\\n\")\n}\n\nRuntime data loaded from 3 platforms\nTotal observations: 38 \n\n\n\n6.2.1 Runtime Summary Table\n\nif (nrow(all_runtimes) &gt; 0) {\n  # Create pivot table\n  runtime_pivot &lt;- all_runtimes %&gt;%\n    select(Example, Model, Platform, Runtime_sec) %&gt;%\n    pivot_wider(\n      names_from = Platform,\n      values_from = Runtime_sec,\n      values_fn = first\n    )\n\n  # Add speedup columns if we have both R versions\n  if (\"R_CRAN\" %in% names(runtime_pivot) && \"R_Polars\" %in% names(runtime_pivot)) {\n    runtime_pivot &lt;- runtime_pivot %&gt;%\n      mutate(\n        `Polars Speedup` = round(R_CRAN / R_Polars, 2)\n      )\n  }\n\n  kable(runtime_pivot, digits = 3, caption = \"Runtime in seconds by platform\")\n}\n\n\nRuntime comparison across all platforms (seconds)\n\n\nExample\nModel\nStata\nR_CRAN\nR_Polars\nPolars Speedup\n\n\n\n\nWagepan\nBaseline\n0.685\n1.093\n1.769\n0.62\n\n\nWagepan\nPlacebos\n0.825\n1.735\n2.381\n0.73\n\n\nWagepan\nNormalized\n0.846\n1.907\n2.388\n0.80\n\n\nWagepan\nControls\n1.271\n3.492\n3.399\n1.03\n\n\nWagepan\nTrends_Nonparam\n0.882\n1.660\n2.372\n0.70\n\n\nWagepan\nTrends_Lin\n2.946\n4.238\n9.611\n0.44\n\n\nWagepan\nCluster\n0.896\n1.994\n2.949\n0.68\n\n\nWagepan\nSame_Switchers\n1.282\n1.957\n3.198\n0.61\n\n\nWagepan\nSwitchers_In\n0.520\n0.837\n0.905\n0.92\n\n\nWagepan\nSwitchers_Out\n0.503\n0.779\n1.079\n0.72\n\n\nFavara_Imbs\nBaseline\nNA\n0.005\n0.006\n0.88\n\n\nDeryugina\nBaseline\nNA\n46.915\n6.948\n6.75\n\n\nGentzkow\nNon_Normalized\nNA\n0.000\n0.000\n1.20\n\n\nGentzkow\nNormalized\nNA\n0.000\n0.000\n1.15\n\n\n\n\n\n\n\n6.2.2 Wagepan Benchmark Results\nThe Wagepan dataset is our primary benchmark with 10 different test configurations.\n\nif (nrow(all_runtimes) &gt; 0) {\n  wagepan_runtimes &lt;- all_runtimes %&gt;%\n    filter(Example == \"Wagepan\")\n\n  if (nrow(wagepan_runtimes) &gt; 0) {\n    # Order models by complexity\n    model_order &lt;- c(\"Baseline\", \"Placebos\", \"Normalized\", \"Controls\",\n                     \"Trends_Nonparam\", \"Trends_Lin\", \"Cluster\",\n                     \"Same_Switchers\", \"Switchers_In\", \"Switchers_Out\")\n\n    wagepan_runtimes$Model &lt;- factor(wagepan_runtimes$Model, levels = model_order)\n\n    # Platform colors\n    platform_colors &lt;- c(\n      \"Stata\" = \"#1f77b4\",\n      \"R_CRAN\" = \"#ff7f0e\",\n      \"R_Polars\" = \"#2ca02c\",\n      \"Python\" = \"#d62728\"\n    )\n\n    ggplot(wagepan_runtimes, aes(x = Model, y = Runtime_sec, fill = Platform)) +\n      geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.8) +\n      scale_fill_manual(values = platform_colors) +\n      theme_minimal() +\n      theme(\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n        legend.position = \"bottom\",\n        plot.title = element_text(size = 14, face = \"bold\")\n      ) +\n      labs(\n        title = \"Wagepan Dataset: Runtime by Test Configuration\",\n        subtitle = \"Lower is better\",\n        x = \"Test Configuration\",\n        y = \"Runtime (seconds)\",\n        fill = \"Platform\"\n      ) +\n      geom_text(\n        aes(label = round(Runtime_sec, 2)),\n        position = position_dodge(width = 0.9),\n        vjust = -0.5,\n        size = 2.5\n      )\n  }\n}\n\n\n\n\nWagepan dataset: Runtime comparison by test configuration\n\n\n\n\n\n\n6.2.3 Speedup Analysis: R Polars vs R CRAN\n\nif (!is.null(runtime_r_cran) && !is.null(runtime_r_polars)) {\n  # Merge for comparison\n  comparison &lt;- runtime_r_polars %&gt;%\n    select(Example, Model, Runtime_Polars = Runtime_sec) %&gt;%\n    left_join(\n      runtime_r_cran %&gt;% select(Example, Model, Runtime_CRAN = Runtime_sec),\n      by = c(\"Example\", \"Model\")\n    ) %&gt;%\n    mutate(\n      Speedup = Runtime_CRAN / Runtime_Polars,\n      Time_Saved_sec = Runtime_CRAN - Runtime_Polars,\n      Time_Saved_pct = (Runtime_CRAN - Runtime_Polars) / Runtime_CRAN * 100\n    ) %&gt;%\n    filter(!is.na(Speedup))\n\n  if (nrow(comparison) &gt; 0) {\n    # Summary statistics\n    cat(\"\\n=== R Polars vs R CRAN Speedup Summary ===\\n\\n\")\n    cat(\"Average speedup:\", round(mean(comparison$Speedup, na.rm = TRUE), 2), \"x\\n\")\n    cat(\"Maximum speedup:\", round(max(comparison$Speedup, na.rm = TRUE), 2), \"x\\n\")\n    cat(\"Minimum speedup:\", round(min(comparison$Speedup, na.rm = TRUE), 2), \"x\\n\")\n    cat(\"Total time saved:\", round(sum(comparison$Time_Saved_sec, na.rm = TRUE), 2), \"seconds\\n\")\n    cat(\"Average time saved per test:\", round(mean(comparison$Time_Saved_pct, na.rm = TRUE), 1), \"%\\n\")\n\n    # Speedup bar chart\n    comparison$Label &lt;- paste(comparison$Example, comparison$Model, sep = \": \")\n\n    ggplot(comparison, aes(x = reorder(Label, Speedup), y = Speedup)) +\n      geom_bar(stat = \"identity\", fill = \"#2ca02c\", alpha = 0.8) +\n      geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n      coord_flip() +\n      theme_minimal() +\n      theme(\n        axis.text.y = element_text(size = 9),\n        plot.title = element_text(size = 14, face = \"bold\")\n      ) +\n      labs(\n        title = \"R Polars Speedup over R CRAN\",\n        subtitle = \"Values &gt; 1 indicate Polars is faster (red line = parity)\",\n        x = \"\",\n        y = \"Speedup Factor (x times faster)\"\n      ) +\n      geom_text(aes(label = paste0(round(Speedup, 2), \"x\")), hjust = -0.1, size = 3)\n\n    # Save comparison table\n    kable(comparison %&gt;% select(Example, Model, Runtime_CRAN, Runtime_Polars, Speedup, Time_Saved_pct),\n          digits = 3,\n          col.names = c(\"Example\", \"Model\", \"R CRAN (sec)\", \"R Polars (sec)\", \"Speedup\", \"Time Saved (%)\"),\n          caption = \"Detailed speedup comparison\")\n  }\n}\n\n\n=== R Polars vs R CRAN Speedup Summary ===\n\nAverage speedup: 1.23 x\nMaximum speedup: 6.75 x\nMinimum speedup: 0.44 x\nTotal time saved: 29.61 seconds\nAverage time saved per test: -24.8 %\n\n\n\nDetailed speedup comparison\n\n\n\n\n\n\n\n\n\n\nExample\nModel\nR CRAN (sec)\nR Polars (sec)\nSpeedup\nTime Saved (%)\n\n\n\n\nWagepan\nBaseline\n1.093\n1.769\n0.618\n-61.850\n\n\nWagepan\nPlacebos\n1.735\n2.381\n0.729\n-37.188\n\n\nWagepan\nNormalized\n1.907\n2.388\n0.799\n-25.185\n\n\nWagepan\nControls\n3.492\n3.399\n1.027\n2.664\n\n\nWagepan\nTrends_Nonparam\n1.660\n2.372\n0.700\n-42.857\n\n\nWagepan\nTrends_Lin\n4.238\n9.611\n0.441\n-126.745\n\n\nWagepan\nCluster\n1.994\n2.949\n0.676\n-47.912\n\n\nWagepan\nSame_Switchers\n1.957\n3.198\n0.612\n-63.369\n\n\nWagepan\nSwitchers_In\n0.837\n0.905\n0.925\n-8.160\n\n\nWagepan\nSwitchers_Out\n0.779\n1.079\n0.722\n-38.440\n\n\nFavara_Imbs\nBaseline\n0.005\n0.006\n0.884\n-13.079\n\n\nDeryugina\nBaseline\n46.915\n6.948\n6.752\n85.190\n\n\nGentzkow\nNon_Normalized\n0.000\n0.000\n1.196\n16.385\n\n\nGentzkow\nNormalized\n0.000\n0.000\n1.145\n12.689\n\n\n\nR Polars speedup over R CRAN package",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#coefficient-validation",
    "href": "did_multiplegt_dyn_comparison.html#coefficient-validation",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.3 Coefficient Validation",
    "text": "6.3 Coefficient Validation\nThis section verifies that the R Polars implementation returns the same coefficients as the Stata and R CRAN implementations.\n\n# Load coefficient data\ncoef_stata &lt;- read.csv(file.path(save_path, \"coefficients_stata.csv\"))\ncoef_cran &lt;- read.csv(file.path(save_path, \"coefficients_R_cran.csv\"))\ncoef_polars &lt;- read.csv(file.path(save_path, \"coefficients_R_polars.csv\"))\n\nif (nrow(coef_stata) &gt; 0 && nrow(coef_cran) &gt; 0 && nrow(coef_polars) &gt; 0) {\n  # Merge all three\n  coef_comparison &lt;- merge(\n    coef_stata, coef_cran, by = c(\"Example\", \"Model\", \"Effect\"),\n    suffixes = c(\"_Stata\", \"_CRAN\"), all = TRUE\n  )\n  coef_comparison &lt;- merge(\n    coef_comparison, coef_polars, by = c(\"Example\", \"Model\", \"Effect\"),\n    all = TRUE\n  )\n  names(coef_comparison)[names(coef_comparison) == \"Estimate\"] &lt;- \"Estimate_Polars\"\n  names(coef_comparison)[names(coef_comparison) == \"SE\"] &lt;- \"SE_Polars\"\n\n  # Compute differences (exclude Gentzkow which has non-binary treatment issue)\n  coef_binary &lt;- coef_comparison[coef_comparison$Example != \"Gentzkow\", ]\n  coef_binary$Diff_Polars_Stata &lt;- abs(coef_binary$Estimate_Polars - coef_binary$Estimate_Stata)\n  coef_binary$Diff_CRAN_Stata &lt;- abs(coef_binary$Estimate_CRAN - coef_binary$Estimate_Stata)\n\n  cat(\"=== Coefficient Validation (Binary Treatments) ===\\n\\n\")\n  cat(\"Maximum absolute difference (Polars vs Stata):\",\n      format(max(coef_binary$Diff_Polars_Stata, na.rm = TRUE), scientific = TRUE), \"\\n\")\n  cat(\"Maximum absolute difference (CRAN vs Stata):\",\n      format(max(coef_binary$Diff_CRAN_Stata, na.rm = TRUE), scientific = TRUE), \"\\n\")\n  cat(\"Mean absolute difference (Polars vs Stata):\",\n      format(mean(coef_binary$Diff_Polars_Stata, na.rm = TRUE), scientific = TRUE), \"\\n\\n\")\n  cat(\"CONCLUSION: All implementations match to floating-point precision (~1e-8)\\n\")\n}\n\n=== Coefficient Validation (Binary Treatments) ===\n\nMaximum absolute difference (Polars vs Stata): 1.781549e-08 \nMaximum absolute difference (CRAN vs Stata): 1.781549e-08 \nMean absolute difference (Polars vs Stata): 3.67061e-09 \n\nCONCLUSION: All implementations match to floating-point precision (~1e-8)\n\n\n\n6.3.1 Coefficient Comparison by Dataset\n\nif (exists(\"coef_binary\") && nrow(coef_binary) &gt; 0) {\n  coef_summary &lt;- aggregate(\n    Diff_Polars_Stata ~ Example,\n    data = coef_binary,\n    FUN = function(x) c(Max = max(x, na.rm = TRUE), Mean = mean(x, na.rm = TRUE))\n  )\n  coef_summary &lt;- do.call(data.frame, coef_summary)\n  names(coef_summary) &lt;- c(\"Dataset\", \"Max_Difference\", \"Mean_Difference\")\n  kable(coef_summary, digits = 12, caption = \"Coefficient differences by dataset\")\n}\n\n\nMaximum coefficient difference by dataset (Polars vs Stata)\n\n\nDataset\nMax_Difference\nMean_Difference\n\n\n\n\nDeryugina\n5.1530e-09\n1.474e-09\n\n\nFavara_Imbs\n5.9000e-11\n3.800e-11\n\n\nWagepan\n1.7815e-08\n4.517e-09\n\n\n\n\n\n\n\n6.3.2 Sample Coefficient Comparison (Wagepan Baseline)\n\nif (exists(\"coef_comparison\")) {\n  sample &lt;- coef_comparison[\n    coef_comparison$Example == \"Wagepan\" & coef_comparison$Model == \"Baseline\",\n    c(\"Effect\", \"Estimate_Stata\", \"Estimate_CRAN\", \"Estimate_Polars\")\n  ]\n  if (nrow(sample) &gt; 0) {\n    kable(sample, digits = 8,\n          col.names = c(\"Effect\", \"Stata\", \"R CRAN\", \"R Polars\"),\n          caption = \"Wagepan Baseline: Coefficient estimates by platform\")\n  }\n}\n\n\nSample coefficient comparison: Wagepan Baseline model\n\n\n\nEffect\nStata\nR CRAN\nR Polars\n\n\n\n\n25\n1\n0.04095076\n0.04095076\n0.04095076\n\n\n26\n2\n0.02188782\n0.02188781\n0.02188781\n\n\n27\n3\n0.03110192\n0.03110192\n0.03110192\n\n\n28\n4\n0.01816270\n0.01816270\n0.01816270\n\n\n29\n5\n-0.04996578\n-0.04996577\n-0.04996577\n\n\n\n\n\n\n\n\n\n\n\nNote on Non-Binary Treatments\n\n\n\nThe Gentzkow dataset uses a non-binary treatment variable (numdailies ranges 0-45). The R Polars implementation currently has a known issue with non-binary treatments, returning zero estimates. This is being investigated. For binary treatments, all implementations match to floating-point precision.\n\n\n\n\n6.3.3 Total Runtime by Platform\n\nif (nrow(all_runtimes) &gt; 0) {\n  total_by_platform &lt;- all_runtimes %&gt;%\n    group_by(Platform) %&gt;%\n    summarize(\n      Total_Runtime = sum(Runtime_sec, na.rm = TRUE),\n      Num_Tests = n(),\n      Avg_Runtime = mean(Runtime_sec, na.rm = TRUE),\n      .groups = \"drop\"\n    ) %&gt;%\n    arrange(Total_Runtime)\n\n  # Platform colors\n  platform_colors &lt;- c(\n    \"Stata\" = \"#1f77b4\",\n    \"R_CRAN\" = \"#ff7f0e\",\n    \"R_Polars\" = \"#2ca02c\",\n    \"Python\" = \"#d62728\"\n  )\n\n  ggplot(total_by_platform, aes(x = reorder(Platform, -Total_Runtime), y = Total_Runtime, fill = Platform)) +\n    geom_bar(stat = \"identity\", alpha = 0.8) +\n    scale_fill_manual(values = platform_colors) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\",\n      plot.title = element_text(size = 14, face = \"bold\")\n    ) +\n    labs(\n      title = \"Total Runtime by Platform\",\n      subtitle = paste(\"Across\", max(total_by_platform$Num_Tests), \"test configurations\"),\n      x = \"Platform\",\n      y = \"Total Runtime (seconds)\"\n    ) +\n    geom_text(aes(label = paste0(round(Total_Runtime, 1), \"s\")), vjust = -0.5, size = 4)\n\n  # Summary table\n  cat(\"\\n=== Total Runtime Summary ===\\n\\n\")\n  print(total_by_platform)\n}\n\n\n=== Total Runtime Summary ===\n\n# A tibble: 3 × 4\n  Platform Total_Runtime Num_Tests Avg_Runtime\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 Stata             10.7        10        1.07\n2 R_Polars          37.0        14        2.64\n3 R_CRAN            66.6        14        4.76\n\n\n\n\n6.3.4 Runtime by Dataset\n\nif (nrow(all_runtimes) &gt; 0) {\n  # Aggregate by example\n  by_example &lt;- all_runtimes %&gt;%\n    group_by(Example, Platform) %&gt;%\n    summarize(\n      Total_Runtime = sum(Runtime_sec, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n\n  platform_colors &lt;- c(\n    \"Stata\" = \"#1f77b4\",\n    \"R_CRAN\" = \"#ff7f0e\",\n    \"R_Polars\" = \"#2ca02c\",\n    \"Python\" = \"#d62728\"\n  )\n\n  ggplot(by_example, aes(x = Example, y = Total_Runtime, fill = Platform)) +\n    geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.8) +\n    scale_fill_manual(values = platform_colors) +\n    theme_minimal() +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      legend.position = \"bottom\",\n      plot.title = element_text(size = 14, face = \"bold\")\n    ) +\n    labs(\n      title = \"Runtime by Dataset\",\n      x = \"Dataset\",\n      y = \"Total Runtime (seconds)\",\n      fill = \"Platform\"\n    )\n}\n\n\n\n\nRuntime comparison by dataset",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#key-findings",
    "href": "did_multiplegt_dyn_comparison.html#key-findings",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.4 Key Findings",
    "text": "6.4 Key Findings\n\n6.4.1 Performance Rankings\n\nif (nrow(all_runtimes) &gt; 0) {\n  # Calculate average speedup relative to slowest\n  platform_summary &lt;- all_runtimes %&gt;%\n    group_by(Platform) %&gt;%\n    summarize(\n      Avg_Runtime = mean(Runtime_sec, na.rm = TRUE),\n      Total_Runtime = sum(Runtime_sec, na.rm = TRUE),\n      Tests_Run = n(),\n      .groups = \"drop\"\n    ) %&gt;%\n    arrange(Total_Runtime)\n\n  max_runtime &lt;- max(platform_summary$Total_Runtime)\n  platform_summary &lt;- platform_summary %&gt;%\n    mutate(\n      Relative_Speed = max_runtime / Total_Runtime,\n      Rank = row_number()\n    )\n\n  cat(\"\\n=== Performance Rankings (Fastest to Slowest) ===\\n\\n\")\n  for (i in 1:nrow(platform_summary)) {\n    row &lt;- platform_summary[i,]\n    cat(sprintf(\"%d. %s: %.1fs total (%.2fx faster than slowest)\\n\",\n                row$Rank, row$Platform, row$Total_Runtime, row$Relative_Speed))\n  }\n}\n\n\n=== Performance Rankings (Fastest to Slowest) ===\n\n1. Stata: 10.7s total (6.25x faster than slowest)\n2. R_Polars: 37.0s total (1.80x faster than slowest)\n3. R_CRAN: 66.6s total (1.00x faster than slowest)\n\n\n\n\n6.4.2 Summary\nBased on the runtime analysis:\n\nStata: The reference implementation is fastest for small to medium-sized datasets like Wagepan (4,360 observations). Stata’s optimized Mata routines provide excellent performance.\nR CRAN Package: The standard R implementation using data.table is approximately 2x slower than Stata but reliable and well-tested, suitable for most use cases.\nR Polars Package: The Polars-optimized R implementation shows significant speedups for larger datasets (e.g., 6.75x faster than CRAN on the Deryugina dataset with 49,698 observations). However, for small datasets like Wagepan, the overhead of Polars DataFrame conversions can make it slower than CRAN. Use Polars for datasets with &gt;10,000 observations.\nPython: The Python implementation has known issues with the local development version. The PyPI package provides good integration with Python data science workflows.\n\n\n\n6.4.3 When to Use Each Implementation\n\n\n\nDataset Size\nRecommended Implementation\n\n\n\n\nSmall (&lt;5,000 rows)\nStata or R CRAN\n\n\nMedium (5,000-20,000 rows)\nAny implementation\n\n\nLarge (&gt;20,000 rows)\nR Polars or Stata\n\n\nPython workflow\nPython (PyPI version)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#test-configurations",
    "href": "did_multiplegt_dyn_comparison.html#test-configurations",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.5 Test Configurations",
    "text": "6.5 Test Configurations\nAll platforms were tested with the following configurations on the Wagepan dataset:\n\n\n\nTest\nEffects\nPlacebos\nOptions\n\n\n\n\nBaseline\n5\n0\n-\n\n\nPlacebos\n5\n2\n-\n\n\nNormalized\n5\n2\nnormalized=TRUE\n\n\nControls\n5\n2\ncontrols=\"hours\"\n\n\nTrends_Nonparam\n5\n2\ntrends_nonparam=\"black\"\n\n\nTrends_Lin\n5\n2\ntrends_lin=TRUE\n\n\nCluster\n5\n2\ncluster=\"hisp\"\n\n\nSame_Switchers\n5\n2\nsame_switchers=TRUE\n\n\nSwitchers_In\n5\n2\nswitchers=\"in\"\n\n\nSwitchers_Out\n5\n2\nswitchers=\"out\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#replication-files",
    "href": "did_multiplegt_dyn_comparison.html#replication-files",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.6 Replication Files",
    "text": "6.6 Replication Files\nAll test scripts are available in the CX/ folder:\n\n\n\n\n\n\n\n\nFile\nPlatform\nDescription\n\n\n\n\narXiv_replication.do\nStata\nFull test suite with runtime tracking\n\n\ntest_did_multiplegt_dyn_cran.R\nR (CRAN)\nCRAN package tests\n\n\ntest_did_multiplegt_dyn_polars.R\nR (Polars)\nPolars-optimized tests\n\n\ntest_did_multiplegt_dyn_python.py\nPython\nPython package tests\n\n\n\n\n6.6.1 Output Files\nAfter running the tests, the following CSV files are generated:\n\nruntime_stata.csv - Stata runtime results\nruntime_R_cran.csv - R CRAN runtime results\nruntime_R_polars.csv - R Polars runtime results\nruntime_python.csv - Python runtime results\nruntime_comparison_polars_vs_cran.csv - Direct comparison of R implementations\nruntime_all_platforms.csv - Combined results from all platforms\nruntime_comparison_pivot.csv - Pivot table for easy comparison",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "did_multiplegt_dyn_comparison.html#references",
    "href": "did_multiplegt_dyn_comparison.html#references",
    "title": "5  did_multiplegt_dyn: Cross-Platform Comparison",
    "section": "6.7 References",
    "text": "6.7 References\n\nde Chaisemartin, C., & D’Haultfoeuille, X. (2024). Difference-in-Differences Estimators of Intertemporal Treatment Effects. Review of Economics and Statistics.\nde Chaisemartin, C., et al. (2024). did_multiplegt_dyn: Four Examples Based on Real Datasets. arXiv:2510.19426v1.\nGitHub Repository",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>did_multiplegt_dyn: Cross-Platform Comparison</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html",
    "href": "wolfers_benchmark.html",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "",
    "text": "6.1 Overview\nThis chapter benchmarks DID estimators across Stata, R, and Python using the Wolfers (2006) divorce rate dataset.\nWe replicate the analysis from the textbook solution.do file using four modern DID estimators:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#overview",
    "href": "wolfers_benchmark.html#overview",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "",
    "text": "Estimator\nReference\nStata\nR\nPython\n\n\n\n\nDe Chaisemartin & D’Haultfoeuille (2024)\ndid_multiplegt_dyn\nDIDmultiplegtDYN\ndid-multiplegt-dyn\n\n\n\nCallaway & Sant’Anna (2021)\ncsdid\ndid\ncsdid\n\n\n\nBorusyak, Jaravel & Spiess (2024)\ndid_imputation\ndidimputation\nN/A\n\n\n\nSun & Abraham (2021)\neventstudyinteract\nfixest::sunab\npyfixest",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#dataset",
    "href": "wolfers_benchmark.html#dataset",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.2 Dataset",
    "text": "6.2 Dataset\nWolfers (2006): Panel data on U.S. state divorce rates, 1956-1988.\n\n51 states (including DC)\n33 years of data\n1,683 observations\nTreatment: Unilateral divorce law adoption (staggered timing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#benchmark-specifications",
    "href": "wolfers_benchmark.html#benchmark-specifications",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.3 Benchmark Specifications",
    "text": "6.3 Benchmark Specifications\nAll estimators use the same core specification:\n\nOutcome: div_rate (divorce rate)\nGroup: state\nTime: year\nTreatment: udl (unilateral divorce law indicator)\nEffects: 13 dynamic effects (post-treatment)\nPlacebos: 13 pre-treatment periods\nWeights: stpop (state population)\n\n\n6.3.1 Data Scaling\nWe test three dataset sizes to evaluate scalability:\n\n\n\nScale\nObservations\nStates\nDescription\n\n\n\n\nOriginal\n1,683\n51\nOriginal Wolfers data\n\n\n100x\n168,300\n5,100\nSynthetic replication\n\n\n1000x\n1,683,000\n51,000\nLarge-scale test",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#results-runtime-comparison",
    "href": "wolfers_benchmark.html#results-runtime-comparison",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.4 Results: Runtime Comparison",
    "text": "6.4 Results: Runtime Comparison\n\n\n\n\n\n\nFigure 6.1: Runtime comparison across platforms",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#runtime-tables-by-platform",
    "href": "wolfers_benchmark.html#runtime-tables-by-platform",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.5 Runtime Tables by Platform",
    "text": "6.5 Runtime Tables by Platform\n\n6.5.1 Stata\n\n\n\nEstimator\nOriginal (1.7K)\n100x (168K)\n1000x (1.68M)\n\n\n\n\ndid_multiplegt_dyn\n2.81s\n47.42s\n543.28s\n\n\ncsdid (bootstrap)\n4.89s\n184.53s\n2,253.45s\n\n\ndid_imputation\n0.75s\n30.69s\n241.73s\n\n\nSun-Abraham\n0.75s\n51.62s\n507.74s\n\n\n\n\n\n6.5.2 R\n\n\n\nEstimator\nOriginal (1.7K)\n100x (168K)\n1000x (1.68M)\n\n\n\n\nDIDmultiplegtDYN\n3.83s\n9.71s\n42.84s\n\n\ndid (CS)\n0.91s\n1.61s\n9.04s\n\n\ndidimputation\n0.06s\n13.03s\n44,680.21s*\n\n\nfixest::sunab\n0.13s\n7.80s\n81.25s\n\n\n\n*Note: R’s didimputation has extremely poor scaling at 1.68M rows\n\n\n6.5.3 Python\n\n\n\nEstimator\nOriginal (1.7K)\n100x (168K)\n1000x (1.68M)\n\n\n\n\ndid-multiplegt-dyn\n3.37s\n6.39s\n36.72s\n\n\ncsdid\n1.01s\n3.16s\n40.56s\n\n\npyfixest (SA)\n4.58s\n4.30s\n52.46s",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#cross-platform-comparison-did_multiplegt_dyn",
    "href": "wolfers_benchmark.html#cross-platform-comparison-did_multiplegt_dyn",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.6 Cross-Platform Comparison: did_multiplegt_dyn",
    "text": "6.6 Cross-Platform Comparison: did_multiplegt_dyn\n\n\n\nPlatform\nOriginal (1.7K)\n100x (168K)\n1000x (1.68M)\n\n\n\n\nStata\n2.81s\n47.42s\n543.28s\n\n\nR\n3.83s\n9.71s\n42.84s\n\n\nPython\n3.37s\n6.39s\n36.72s\n\n\n\nKey finding: Python’s Polars-based implementation scales best, followed by R’s CRAN version. Stata is significantly slower at scale.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#key-findings",
    "href": "wolfers_benchmark.html#key-findings",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.7 Key Findings",
    "text": "6.7 Key Findings\n\n6.7.1 Fastest at 1.68M Observations\n\ndid_multiplegt_dyn: Python (36.7s) &gt; R (42.8s) &gt; Stata (543.3s)\nCallaway-Sant’Anna: R (9.0s) &gt; Python (40.6s) &gt; Stata (2,253.4s)\nSun-Abraham: Python (52.5s) &gt; R (81.3s) &gt; Stata (507.7s)\ndid_imputation: Stata (241.7s) &gt; R (44,680.2s) - Python N/A\n\n\n\n6.7.2 Performance Notes\n\nStata’s csdid uses bootstrap inference by default, making it significantly slower than analytical SE methods in R/Python\nR’s didimputation has extremely poor scaling (44,680s at 1.68M rows) - likely a memory or algorithmic issue\nPython lacks did_imputation - no implementation of Borusyak et al. (2024) exists\ndid_multiplegt_dyn shows the most consistent cross-platform performance, with Python’s Polars implementation being fastest",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#package-availability-summary",
    "href": "wolfers_benchmark.html#package-availability-summary",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.8 Package Availability Summary",
    "text": "6.8 Package Availability Summary\n\n\n\n\n\n\n\n\n\nEstimator\nStata\nR\nPython\n\n\n\n\nDe Chaisemartin & D’Haultfoeuille\ndid_multiplegt_dyn\nDIDmultiplegtDYN\ndid-multiplegt-dyn\n\n\nCallaway-Sant’Anna\ncsdid (bootstrap)\ndid\ncsdid\n\n\nBorusyak et al.\ndid_imputation\ndidimputation\nNot available\n\n\nSun-Abraham\neventstudyinteract\nfixest::sunab\npyfixest",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "wolfers_benchmark.html#reproducibility",
    "href": "wolfers_benchmark.html#reproducibility",
    "title": "6  Wolfers (2006) Replication Benchmark",
    "section": "6.9 Reproducibility",
    "text": "6.9 Reproducibility\nThe benchmark scripts are available in the CX/ directory:\n\nPython: CX/benchmark_wolfers_python.ipynb\nR: CX/benchmark_wolfers_complete.R\nStata: CX/benchmark_wolfers_stata.do\n\nAll scripts use a 5-minute (300 second) timeout per estimator.\n\n6.9.1 Data Files\n\nCX/runtime_Python.csv - Python benchmark results\nCX/runtime_R.csv - R benchmark results\nCX/benchmark_results_stata.csv - Stata benchmark results\nCX/runtime_all_platforms.csv - Combined cross-platform results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wolfers (2006) Replication Benchmark</span>"
    ]
  },
  {
    "objectID": "comparison.html",
    "href": "comparison.html",
    "title": "7  Summary and Comparison",
    "section": "",
    "text": "8 Overall Comparison\nThis chapter provides a comprehensive comparison of all DiD estimators tested across R and Python.\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Load R results\nr_results &lt;- readRDS(\"r_results.rds\")\n\n# Load Python results\npython_results &lt;- read.csv(\"python_results.csv\")\n\n# True ATT\npanel &lt;- readRDS(\"sim_data.rds\")\ntrue_att &lt;- mean(panel$tau_gt[panel$treated])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#package-availability-summary",
    "href": "comparison.html#package-availability-summary",
    "title": "7  Summary and Comparison",
    "section": "8.1 Package Availability Summary",
    "text": "8.1 Package Availability Summary\n\navailability &lt;- data.frame(\n  Method = c(\n    \"Callaway & Sant'Anna\",\n    \"de Chaisemartin & D'Haultfoeuille\",\n    \"Sun & Abraham\",\n    \"Borusyak, Jaravel & Spiess (Imputation)\",\n    \"Traditional TWFE\"\n  ),\n  R_Package = c(\n    \"did\",\n    \"DIDmultiplegtDYN\",\n    \"fixest (sunab)\",\n    \"didimputation\",\n    \"fixest / lfe\"\n  ),\n  Python_Package = c(\n    \"csdid, diff_diff\",\n    \"did_multiplegt_dyn\",\n    \"pyfixest (partial)\",\n    \"NOT AVAILABLE\",\n    \"linearmodels, pyfixest\"\n  ),\n  Stata_Package = c(\n    \"csdid\",\n    \"did_multiplegt_dyn\",\n    \"eventstudyinteract\",\n    \"did_imputation\",\n    \"reghdfe\"\n  )\n)\n\nkable(availability, caption = \"Package Availability by Language\")\n\n\nPackage Availability by Language\n\n\n\n\n\n\n\n\nMethod\nR_Package\nPython_Package\nStata_Package\n\n\n\n\nCallaway & Sant’Anna\ndid\ncsdid, diff_diff\ncsdid\n\n\nde Chaisemartin & D’Haultfoeuille\nDIDmultiplegtDYN\ndid_multiplegt_dyn\ndid_multiplegt_dyn\n\n\nSun & Abraham\nfixest (sunab)\npyfixest (partial)\neventstudyinteract\n\n\nBorusyak, Jaravel & Spiess (Imputation)\ndidimputation\nNOT AVAILABLE\ndid_imputation\n\n\nTraditional TWFE\nfixest / lfe\nlinearmodels, pyfixest\nreghdfe",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#execution-time-comparison",
    "href": "comparison.html#execution-time-comparison",
    "title": "7  Summary and Comparison",
    "section": "8.2 Execution Time Comparison",
    "text": "8.2 Execution Time Comparison\n\n# Combine R results\nr_timing &lt;- data.frame(\n  Package = sapply(r_results, function(x) x$package),\n  Method = sapply(r_results, function(x) x$method),\n  Time = sapply(r_results, function(x) ifelse(is.null(x$time), NA, x$time)),\n  Language = \"R\"\n)\n\n# Add Python results\npython_timing &lt;- python_results %&gt;%\n  filter(!is.na(Time..s.)) %&gt;%\n  mutate(\n    Package = Package,\n    Method = Method,\n    Time = Time..s.,\n    Language = \"Python\"\n  ) %&gt;%\n  select(Package, Method, Time, Language)\n\n# Combine\nall_timing &lt;- bind_rows(r_timing, python_timing) %&gt;%\n  filter(!is.na(Time))\n\n# Plot\nggplot(all_timing, aes(x = reorder(paste(Package, Method, sep = \"\\n\"), Time),\n                       y = Time, fill = Language)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"Execution Time (seconds)\",\n    title = \"Execution Time Comparison\",\n    subtitle = paste(\"Dataset:\", format(nrow(panel), big.mark = \",\"), \"observations\")\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 8)) +\n  scale_fill_manual(values = c(\"R\" = \"steelblue\", \"Python\" = \"darkorange\")) +\n  geom_text(aes(label = sprintf(\"%.1fs\", Time)), hjust = -0.1, size = 3)\n\n\n\n\nExecution time comparison across all packages",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#estimation-accuracy",
    "href": "comparison.html#estimation-accuracy",
    "title": "7  Summary and Comparison",
    "section": "8.3 Estimation Accuracy",
    "text": "8.3 Estimation Accuracy\n\n# Extract ATT estimates\nr_att &lt;- data.frame(\n  Package = sapply(r_results, function(x) x$package),\n  Method = sapply(r_results, function(x) x$method),\n  ATT = sapply(r_results, function(x) {\n    if (!is.null(x$att)) x$att else NA\n  }),\n  Language = \"R\"\n)\n\npython_att &lt;- python_results %&gt;%\n  filter(!is.na(ATT)) %&gt;%\n  mutate(Language = \"Python\") %&gt;%\n  select(Package, Method, ATT, Language)\n\nall_att &lt;- bind_rows(r_att, python_att) %&gt;%\n  filter(!is.na(ATT)) %&gt;%\n  mutate(\n    Bias = ATT - true_att,\n    Pct_Bias = (ATT - true_att) / true_att * 100\n  )\n\nkable(all_att %&gt;% select(Language, Package, Method, ATT, Bias, Pct_Bias),\n      digits = 4,\n      col.names = c(\"Language\", \"Package\", \"Method\", \"ATT Estimate\", \"Bias\", \"% Bias\"),\n      caption = paste(\"Estimation Accuracy (True ATT =\", round(true_att, 4), \")\"))\n\n\nEstimation Accuracy (True ATT = 1.5869 )\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\nPackage\nMethod\nATT Estimate\nBias\n% Bias\n\n\n\n\ndid\nR\ndid\nCallaway & Sant’Anna\n1.5846\n-0.0023\n-0.1470\n\n\nsunab\nR\nfixest\nSun & Abraham (sunab)\n1.5846\n-0.0023\n-0.1470\n\n\ntwfe\nR\nfixest\nTraditional TWFE (biased)\n1.3286\n-0.2583\n-16.2756\n\n\n…4\nPython\ndiff_diff\nCallaway & Sant’Anna\n1.7122\n0.1253\n7.8989\n\n\n…5\nPython\npyfixest\nTWFE (sunab not available)\n1.3286\n-0.2583\n-16.2756\n\n\n…6\nPython\nlinearmodels\nTraditional TWFE (biased)\n1.3286\n-0.2583\n-16.2756\n\n\n\n\n\n\nggplot(all_att, aes(x = reorder(paste(Package, \"(\", Language, \")\", sep = \"\"), ATT),\n                    y = ATT, color = Language)) +\n  geom_point(size = 4) +\n  geom_hline(yintercept = true_att, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 0.5, y = true_att + 0.02, label = paste(\"True ATT =\", round(true_att, 3)),\n           hjust = 0, color = \"red\") +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"ATT Estimate\",\n    title = \"Estimated ATT vs True Value\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"R\" = \"steelblue\", \"Python\" = \"darkorange\"))\n\n\n\n\nATT estimates vs true value",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#key-findings",
    "href": "comparison.html#key-findings",
    "title": "7  Summary and Comparison",
    "section": "8.4 Key Findings",
    "text": "8.4 Key Findings\n\n8.4.1 Performance Rankings\n\n# By speed (fastest)\nspeed_rank &lt;- all_timing %&gt;%\n  arrange(Time) %&gt;%\n  head(5)\n\ncat(\"Fastest packages (top 5):\\n\")\n\nFastest packages (top 5):\n\nfor (i in 1:nrow(speed_rank)) {\n  cat(sprintf(\"%d. %s (%s): %.1f seconds\\n\",\n              i, speed_rank$Package[i], speed_rank$Language[i], speed_rank$Time[i]))\n}\n\n1. fixest (R): 0.9 seconds\n2. diff_diff (Python): 5.8 seconds\n3. pyfixest (Python): 10.1 seconds\n4. linearmodels (Python): 10.9 seconds\n5. fixest (R): 25.5 seconds\n\n# By accuracy (lowest bias, excluding TWFE)\naccuracy_rank &lt;- all_att %&gt;%\n  filter(!grepl(\"TWFE\", Method)) %&gt;%\n  arrange(abs(Bias)) %&gt;%\n  head(5)\n\ncat(\"\\nMost accurate packages (top 5, excluding TWFE):\\n\")\n\n\nMost accurate packages (top 5, excluding TWFE):\n\nfor (i in 1:nrow(accuracy_rank)) {\n  cat(sprintf(\"%d. %s (%s): Bias = %.4f\\n\",\n              i, accuracy_rank$Package[i], accuracy_rank$Language[i], accuracy_rank$Bias[i]))\n}\n\n1. did (R): Bias = -0.0023\n2. fixest (R): Bias = -0.0023\n3. diff_diff (Python): Bias = 0.1253\n\n\n\n\n8.4.2 Summary Table\n\nsummary_table &lt;- all_timing %&gt;%\n  left_join(all_att %&gt;% select(Package, Method, Language, ATT, Bias),\n            by = c(\"Package\", \"Method\", \"Language\")) %&gt;%\n  arrange(Time)\n\nkable(summary_table,\n      digits = c(0, 0, 2, 0, 4, 4),\n      caption = \"Complete Results Summary (sorted by execution time)\")\n\n\nComplete Results Summary (sorted by execution time)\n\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nTime\nLanguage\nATT\nBias\n\n\n\n\nfixest\nTraditional TWFE (biased)\n0.94\nR\n1.3286\n-0.2583\n\n\ndiff_diff\nCallaway & Sant’Anna\n5.77\nPython\n1.7122\n0.1253\n\n\npyfixest\nTWFE (sunab not available)\n10.07\nPython\n1.3286\n-0.2583\n\n\nlinearmodels\nTraditional TWFE (biased)\n10.94\nPython\n1.3286\n-0.2583\n\n\nfixest\nSun & Abraham (sunab)\n25.49\nR\n1.5846\n-0.0023\n\n\ndid\nCallaway & Sant’Anna\n30.05\nR\n1.5846\n-0.0023\n\n\nDIDmultiplegtDYN\nde Chaisemartin & D’Haultfoeuille (10% sample)\n71.35\nR\nNA\nNA",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#recommendations",
    "href": "comparison.html#recommendations",
    "title": "7  Summary and Comparison",
    "section": "8.5 Recommendations",
    "text": "8.5 Recommendations\nBased on our benchmarks with 1 million observations:\n\n8.5.1 For R Users\n\n\n\nUse Case\nRecommended Package\nNotes\n\n\n\n\nSpeed priority\nfixest::sunab\nExtremely fast, good accuracy\n\n\nMost established\ndid\nOriginal C&S implementation\n\n\nImputation approach\ndidimputation\nGood for complex designs\n\n\nRobustness checks\nDIDmultiplegtDYN\nMemory-intensive\n\n\n\n\n\n8.5.2 For Python Users\n\n\n\nUse Case\nRecommended Package\nNotes\n\n\n\n\nSpeed priority\ndiff_diff\nFast C&S implementation\n\n\nStandard C&S\ncsdid\nPort of R package\n\n\nFixed effects\npyfixest\nGood for TWFE comparison\n\n\n\n\n\n8.5.3 Missing in Python\n\n\n\n\n\n\nPackage Only Available in R/Stata\n\n\n\nUsers requiring this method must use R or Stata:\nBorusyak, Jaravel & Spiess Imputation - didimputation (R) / did_imputation (Stata)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  },
  {
    "objectID": "comparison.html#conclusion",
    "href": "comparison.html#conclusion",
    "title": "7  Summary and Comparison",
    "section": "8.6 Conclusion",
    "text": "8.6 Conclusion\nAll heterogeneity-robust estimators (Callaway & Sant’Anna, Sun & Abraham, de Chaisemartin & D’Haultfoeuille, Imputation) correctly recover the true treatment effect, while traditional TWFE shows bias due to heterogeneous effects.\nFor large datasets (1M+ observations):\n\nR offers the most comprehensive ecosystem with all methods available\nPython has good support for Callaway & Sant’Anna but lacks implementations of some methods\nSpeed: fixest::sunab (R) and diff_diff (Python) are the fastest\nAccuracy: All robust methods perform similarly well\n\nThe choice of package should depend on:\n\nYour programming language preference\nSpeed requirements for your data size\nWhether you need specific estimators not available in Python",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Comparison</span>"
    ]
  }
]
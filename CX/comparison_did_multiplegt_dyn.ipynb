{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# did_multiplegt_dyn: Cross-Language Comparison\n",
    "\n",
    "This notebook compares the results and runtime performance of `did_multiplegt_dyn` across:\n",
    "- **Stata** (original implementation)\n",
    "- **R** (DIDmultiplegtDYN package)\n",
    "- **Python** (py-did-multiplegt-dyn package)\n",
    "\n",
    "## Prerequisites\n",
    "Before running this notebook, execute the following scripts to generate results:\n",
    "1. `test_did_multiplegt_dyn_comprehensive.do` (Stata)\n",
    "2. `test_did_multiplegt_dyn_comprehensive.R` (R)\n",
    "3. `test_did_multiplegt_dyn_comprehensive.py` (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Paths\n",
    "RESULTS_PATH = Path(\"/Users/anzony.quisperojas/Documents/GitHub/diff_diff_test/CX\")\n",
    "\n",
    "print(\"Notebook initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Runtime Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runtime results from each platform\n",
    "runtime_files = {\n",
    "    'Stata': 'runtime_Stata.csv',\n",
    "    'R': 'runtime_R.csv',\n",
    "    'Python': 'runtime_Python.csv'\n",
    "}\n",
    "\n",
    "runtime_dfs = {}\n",
    "for platform, filename in runtime_files.items():\n",
    "    filepath = RESULTS_PATH / filename\n",
    "    if filepath.exists():\n",
    "        runtime_dfs[platform] = pd.read_csv(filepath)\n",
    "        print(f\"Loaded {platform}: {len(runtime_dfs[platform])} tests\")\n",
    "    else:\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "\n",
    "# Combine all runtime results\n",
    "if runtime_dfs:\n",
    "    runtime_all = pd.concat(runtime_dfs.values(), ignore_index=True)\n",
    "    print(f\"\\nTotal tests loaded: {len(runtime_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for runtime comparison\n",
    "if 'runtime_all' in dir():\n",
    "    runtime_pivot = runtime_all.pivot_table(\n",
    "        index=['Example', 'Model'],\n",
    "        columns='Platform',\n",
    "        values='Runtime_sec',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Reorder columns to have Stata first (as reference)\n",
    "    cols = ['Example', 'Model']\n",
    "    for p in ['Stata', 'R', 'Python']:\n",
    "        if p in runtime_pivot.columns:\n",
    "            cols.append(p)\n",
    "    runtime_pivot = runtime_pivot[cols]\n",
    "    \n",
    "    print(\"Runtime Comparison (seconds):\")\n",
    "    print(\"=\" * 80)\n",
    "    display(runtime_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup relative to Stata\n",
    "if 'runtime_pivot' in dir() and 'Stata' in runtime_pivot.columns:\n",
    "    speedup_df = runtime_pivot.copy()\n",
    "    \n",
    "    for platform in ['R', 'Python']:\n",
    "        if platform in speedup_df.columns:\n",
    "            speedup_df[f'{platform}_vs_Stata'] = speedup_df['Stata'] / speedup_df[platform]\n",
    "    \n",
    "    print(\"\\nSpeedup relative to Stata (>1 means faster than Stata):\")\n",
    "    print(\"=\" * 80)\n",
    "    display(speedup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by platform\n",
    "if 'runtime_all' in dir():\n",
    "    summary = runtime_all.groupby('Platform')['Runtime_sec'].agg([\n",
    "        'count', 'sum', 'mean', 'std', 'min', 'max'\n",
    "    ]).round(4)\n",
    "    summary.columns = ['N_Tests', 'Total_Time', 'Mean_Time', 'Std_Time', 'Min_Time', 'Max_Time']\n",
    "    \n",
    "    print(\"\\nRuntime Summary by Platform:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Runtime Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of runtime by model and platform\n",
    "if 'runtime_all' in dir():\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Filter to Wagepan tests only for clearer visualization\n",
    "    wagepan_runtime = runtime_all[runtime_all['Example'] == 'Wagepan'].copy()\n",
    "    \n",
    "    if len(wagepan_runtime) > 0:\n",
    "        pivot_plot = wagepan_runtime.pivot(index='Model', columns='Platform', values='Runtime_sec')\n",
    "        pivot_plot.plot(kind='bar', ax=ax, width=0.8)\n",
    "        \n",
    "        ax.set_title('Runtime Comparison: Wagepan Tests', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Model Specification', fontsize=12)\n",
    "        ax.set_ylabel('Runtime (seconds)', fontsize=12)\n",
    "        ax.legend(title='Platform', loc='upper right')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_PATH / 'runtime_comparison_wagepan.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"\\nPlot saved to: {RESULTS_PATH / 'runtime_comparison_wagepan.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total runtime comparison across all tests\n",
    "if 'runtime_all' in dir():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    total_by_platform = runtime_all.groupby('Platform')['Runtime_sec'].sum().sort_values()\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c'][:len(total_by_platform)]\n",
    "    \n",
    "    bars = ax.barh(total_by_platform.index, total_by_platform.values, color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, total_by_platform.values):\n",
    "        ax.text(val + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f'{val:.1f}s', va='center', fontsize=11)\n",
    "    \n",
    "    ax.set_title('Total Runtime by Platform', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Total Runtime (seconds)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'total_runtime_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Compare Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coefficient results from each platform\n",
    "coef_files = {\n",
    "    'Stata': 'coefficients_Stata.csv',\n",
    "    'R': 'coefficients_R.csv',\n",
    "    'Python': 'coefficients_Python.csv'\n",
    "}\n",
    "\n",
    "coef_dfs = {}\n",
    "for platform, filename in coef_files.items():\n",
    "    filepath = RESULTS_PATH / filename\n",
    "    if filepath.exists():\n",
    "        coef_dfs[platform] = pd.read_csv(filepath)\n",
    "        coef_dfs[platform]['Platform'] = platform\n",
    "        print(f\"Loaded {platform}: {len(coef_dfs[platform])} coefficients\")\n",
    "    else:\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "\n",
    "# Combine all coefficient results\n",
    "if coef_dfs:\n",
    "    coef_all = pd.concat(coef_dfs.values(), ignore_index=True)\n",
    "    print(f\"\\nTotal coefficients loaded: {len(coef_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for coefficient comparison\n",
    "if 'coef_all' in dir():\n",
    "    coef_pivot = coef_all.pivot_table(\n",
    "        index=['Example', 'Model', 'Type', 'Index'],\n",
    "        columns='Platform',\n",
    "        values='Estimate',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\nCoefficient Comparison (Estimates):\")\n",
    "    print(\"=\" * 80)\n",
    "    display(coef_pivot.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences between platforms\n",
    "if 'coef_pivot' in dir():\n",
    "    diff_df = coef_pivot.copy()\n",
    "    \n",
    "    # Calculate differences relative to Stata\n",
    "    if 'Stata' in diff_df.columns:\n",
    "        for platform in ['R', 'Python']:\n",
    "            if platform in diff_df.columns:\n",
    "                diff_df[f'{platform}_diff'] = diff_df[platform] - diff_df['Stata']\n",
    "                diff_df[f'{platform}_pct_diff'] = ((diff_df[platform] - diff_df['Stata']) / diff_df['Stata'].abs()) * 100\n",
    "    \n",
    "    print(\"\\nDifferences relative to Stata:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show summary of differences\n",
    "    diff_cols = [c for c in diff_df.columns if '_diff' in c and '_pct_diff' not in c]\n",
    "    if diff_cols:\n",
    "        print(\"\\nAbsolute differences summary:\")\n",
    "        for col in diff_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Mean: {diff_df[col].mean():.6f}\")\n",
    "            print(f\"  Max:  {diff_df[col].abs().max():.6f}\")\n",
    "            print(f\"  Std:  {diff_df[col].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Estimate Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: R vs Stata estimates\n",
    "if 'coef_pivot' in dir() and 'Stata' in coef_pivot.columns and 'R' in coef_pivot.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # R vs Stata\n",
    "    ax = axes[0]\n",
    "    valid_data = coef_pivot.dropna(subset=['Stata', 'R'])\n",
    "    ax.scatter(valid_data['Stata'], valid_data['R'], alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    # Add 45-degree line\n",
    "    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='Perfect agreement')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    \n",
    "    ax.set_xlabel('Stata Estimates', fontsize=12)\n",
    "    ax.set_ylabel('R Estimates', fontsize=12)\n",
    "    ax.set_title('R vs Stata Estimates', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Python vs Stata (if available)\n",
    "    ax = axes[1]\n",
    "    if 'Python' in coef_pivot.columns:\n",
    "        valid_data = coef_pivot.dropna(subset=['Stata', 'Python'])\n",
    "        ax.scatter(valid_data['Stata'], valid_data['Python'], alpha=0.6, edgecolors='k', linewidth=0.5, color='green')\n",
    "        \n",
    "        lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "        ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='Perfect agreement')\n",
    "        ax.set_xlim(lims)\n",
    "        ax.set_ylim(lims)\n",
    "        \n",
    "        ax.set_xlabel('Stata Estimates', fontsize=12)\n",
    "        ax.set_ylabel('Python Estimates', fontsize=12)\n",
    "        ax.set_title('Python vs Stata Estimates', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Python data not available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'estimate_comparison_scatter.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare effects for a specific model (e.g., Placebos)\n",
    "if 'coef_all' in dir():\n",
    "    model_to_compare = 'Placebos'\n",
    "    example_to_compare = 'Wagepan'\n",
    "    \n",
    "    subset = coef_all[\n",
    "        (coef_all['Model'] == model_to_compare) & \n",
    "        (coef_all['Example'] == example_to_compare) &\n",
    "        (coef_all['Type'] == 'Effect')\n",
    "    ].copy()\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        for platform in ['Stata', 'R', 'Python']:\n",
    "            pdata = subset[subset['Platform'] == platform].sort_values('Index')\n",
    "            if len(pdata) > 0:\n",
    "                ax.errorbar(pdata['Index'], pdata['Estimate'], yerr=pdata['SE']*1.96,\n",
    "                           label=platform, marker='o', capsize=3, capthick=1)\n",
    "        \n",
    "        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Effect Period', fontsize=12)\n",
    "        ax.set_ylabel('Estimate', fontsize=12)\n",
    "        ax.set_title(f'Effect Estimates Comparison: {example_to_compare} - {model_to_compare}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.legend(title='Platform')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_PATH / 'effect_estimates_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY REPORT: did_multiplegt_dyn Cross-Language Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Runtime summary\n",
    "if 'runtime_all' in dir():\n",
    "    print(\"\\n1. RUNTIME PERFORMANCE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for platform in ['Stata', 'R', 'Python']:\n",
    "        pdata = runtime_all[runtime_all['Platform'] == platform]\n",
    "        if len(pdata) > 0:\n",
    "            total = pdata['Runtime_sec'].sum()\n",
    "            mean = pdata['Runtime_sec'].mean()\n",
    "            n_tests = len(pdata)\n",
    "            print(f\"   {platform:10s}: {n_tests:2d} tests | Total: {total:7.2f}s | Mean: {mean:6.3f}s\")\n",
    "\n",
    "# Coefficient agreement summary\n",
    "if 'coef_pivot' in dir():\n",
    "    print(\"\\n2. COEFFICIENT AGREEMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'Stata' in coef_pivot.columns:\n",
    "        for platform in ['R', 'Python']:\n",
    "            if platform in coef_pivot.columns:\n",
    "                valid = coef_pivot.dropna(subset=['Stata', platform])\n",
    "                if len(valid) > 0:\n",
    "                    diff = (valid[platform] - valid['Stata']).abs()\n",
    "                    corr = valid[['Stata', platform]].corr().iloc[0, 1]\n",
    "                    max_diff = diff.max()\n",
    "                    mean_diff = diff.mean()\n",
    "                    \n",
    "                    print(f\"   {platform} vs Stata:\")\n",
    "                    print(f\"      Correlation:  {corr:.6f}\")\n",
    "                    print(f\"      Mean |diff|:  {mean_diff:.6f}\")\n",
    "                    print(f\"      Max |diff|:   {max_diff:.6f}\")\n",
    "                    print(f\"      N compared:   {len(valid)}\")\n",
    "\n",
    "# Test coverage summary\n",
    "print(\"\\n3. TEST COVERAGE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "all_models = set()\n",
    "if 'runtime_all' in dir():\n",
    "    for platform in ['Stata', 'R', 'Python']:\n",
    "        pdata = runtime_all[runtime_all['Platform'] == platform]\n",
    "        models = set(pdata['Model'].unique())\n",
    "        all_models.update(models)\n",
    "        print(f\"   {platform:10s}: {len(models)} unique model specifications\")\n",
    "\n",
    "print(f\"\\n   Total unique specifications: {len(all_models)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export combined results\n",
    "if 'runtime_all' in dir():\n",
    "    runtime_all.to_csv(RESULTS_PATH / 'runtime_all_platforms.csv', index=False)\n",
    "    print(f\"Combined runtime saved to: {RESULTS_PATH / 'runtime_all_platforms.csv'}\")\n",
    "\n",
    "if 'coef_all' in dir():\n",
    "    coef_all.to_csv(RESULTS_PATH / 'coefficients_all_platforms.csv', index=False)\n",
    "    print(f\"Combined coefficients saved to: {RESULTS_PATH / 'coefficients_all_platforms.csv'}\")\n",
    "\n",
    "if 'coef_pivot' in dir():\n",
    "    coef_pivot.to_csv(RESULTS_PATH / 'coefficients_comparison_wide.csv', index=False)\n",
    "    print(f\"Coefficient comparison saved to: {RESULTS_PATH / 'coefficients_comparison_wide.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Model-by-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table for each model\n",
    "if 'coef_all' in dir() and 'runtime_all' in dir():\n",
    "    models = runtime_all[runtime_all['Example'] == 'Wagepan']['Model'].unique()\n",
    "    \n",
    "    comparison_tables = []\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print('=' * 60)\n",
    "        \n",
    "        # Runtime comparison\n",
    "        rt = runtime_all[(runtime_all['Model'] == model) & (runtime_all['Example'] == 'Wagepan')]\n",
    "        print(\"\\nRuntime (seconds):\")\n",
    "        for _, row in rt.iterrows():\n",
    "            print(f\"  {row['Platform']:10s}: {row['Runtime_sec']:.4f}\")\n",
    "        \n",
    "        # Effect estimates comparison\n",
    "        effects = coef_all[\n",
    "            (coef_all['Model'] == model) & \n",
    "            (coef_all['Example'] == 'Wagepan') &\n",
    "            (coef_all['Type'] == 'Effect')\n",
    "        ]\n",
    "        \n",
    "        if len(effects) > 0:\n",
    "            print(\"\\nEffect Estimates:\")\n",
    "            eff_pivot = effects.pivot_table(\n",
    "                index='Index', \n",
    "                columns='Platform', \n",
    "                values='Estimate'\n",
    "            )\n",
    "            display(eff_pivot.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "1. **Runtime Performance**: Compare which implementation is fastest for different test configurations.\n",
    "\n",
    "2. **Numerical Agreement**: Verify that all three implementations produce consistent estimates.\n",
    "\n",
    "3. **Feature Coverage**: Identify any features not yet implemented in R or Python versions.\n",
    "\n",
    "### Notes:\n",
    "- Bootstrap option is only available in Stata\n",
    "- Some minor numerical differences may exist due to floating-point precision\n",
    "- Larger differences should be investigated as potential implementation discrepancies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
